{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2024 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ],
   "metadata": {
    "id": "32yCsRUo8H33"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ],
   "metadata": {
    "id": "XCybYoGz8YWQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "6po98qVA8bJD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# define necessary libraries\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data reading\n",
    "\n",
    "train_claims = json.load(open(\"data/train-claims.json\", \"r\"))\n",
    "dev_claims = json.load(open(\"data/dev-claims.json\", \"r\"))\n",
    "test_claims = json.load(open(\"data/test-claims-unlabelled.json\", \"r\"))\n",
    "evidences = json.load(open(\"data/evidence.json\", \"r\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma != word:\n",
    "        return lemma\n",
    "    return lemmatizer.lemmatize(word, 'n')\n",
    "\n",
    "def lemmatize_remove_stop_words(text):\n",
    "    words = text.lower().split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatize_word(w)\n",
    "        if lemma not in stopwords_list:\n",
    "            new_words.append(lemma)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "# Process evidences\n",
    "evidences_texts = [lemmatize_remove_stop_words(evidence_text) for _, evidence_text in evidences.items()]\n",
    "evidences_ids = list(evidences.keys())\n",
    "evidences_id_dict = {evidence_id: idx for idx, evidence_id in enumerate(evidences_ids)}\n",
    "\n",
    "# Process train set\n",
    "train_ids = list(train_claims.keys())\n",
    "train_texts = [lemmatize_remove_stop_words(data[\"claim_text\"]) for _, data in train_claims.items()]\n",
    "train_labels = [data[\"claim_label\"] for _, data in train_claims.items()]\n",
    "train_evidences = [[evidences_id_dict[i] for i in data[\"evidences\"]] for _, data in train_claims.items()]\n",
    "\n",
    "# Process dev set\n",
    "dev_ids = list(dev_claims.keys())\n",
    "dev_texts = [lemmatize_remove_stop_words(data[\"claim_text\"]) for _, data in dev_claims.items()]\n",
    "dev_labels = [data[\"claim_label\"] for _, data in dev_claims.items()]\n",
    "dev_evidences = [[evidences_id_dict[i] for i in data[\"evidences\"]] for _, data in dev_claims.items()]\n",
    "\n",
    "# Process test set\n",
    "test_ids = list(test_claims.keys())\n",
    "test_texts = [lemmatize_remove_stop_words(j[\"claim_text\"]) for _, j in test_claims.items()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(evidences_texts)\n",
    "\n",
    "train_tfidf = vectorizer.transform(train_texts)\n",
    "dev_tfidf = vectorizer.transform(dev_texts)\n",
    "test_tfidf = vectorizer.transform(test_texts)\n",
    "evidence_tfidf = vectorizer.transform(evidences_texts)\n",
    "\n",
    "print(train_tfidf.shape)\n",
    "print(evidence_tfidf.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_cos_sims = np.dot(train_tfidf, evidence_tfidf.transpose()).toarray()\n",
    "dev_cos_sims = np.dot(dev_tfidf, evidence_tfidf.transpose()).toarray()\n",
    "test_cos_sims = np.dot(test_tfidf, evidence_tfidf.transpose()).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_retrieval_topk(k, cur_scores, cur_labels):\n",
    "    ACC = []\n",
    "    top_ids = torch.topk(torch.FloatTensor(cur_scores), k, -1).indices.tolist()\n",
    "\n",
    "    for i in range(len(cur_labels)):\n",
    "        all_count = 0\n",
    "        recall_count = 0\n",
    "        for cur_ in cur_labels[i]:\n",
    "            if cur_ in top_ids[i]:\n",
    "                recall_count += 1\n",
    "            all_count += 1\n",
    "        ACC.append(recall_count / all_count)\n",
    "    print(sum(ACC) / len(ACC))\n",
    "\n",
    "topK = 10\n",
    "test_retrieval_topk(topK, train_cos_sims, train_evidences)\n",
    "test_retrieval_topk(topK, dev_cos_sims, dev_evidences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "1FA2ao2l8hOg"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "QIEqDDT78q39"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "EzGuzHPE87Ya"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6ZVeNYIH9IaL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ],
   "metadata": {
    "id": "mefSOe8eTmGP"
   }
  }
 ]
}
