{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcWPFMqtJ4x",
        "outputId": "f6d7c683-5add-493b-cd7f-5688373f6e9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohyt_MA86_7e",
        "outputId": "3a7cdb86-477e-4687-9151-7945612a1e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "train_claim_labels = train_claims['claim_label']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_claim_labels = dev_claims['claim_label']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEmCnO6TkYRn"
      },
      "outputs": [],
      "source": [
        "test_claims = pd.read_json('/content/drive/MyDrive/nlp/data/test-claims-unlabelled.json', orient='index')\n",
        "test_claims.reset_index(inplace=True)\n",
        "test_claims.columns = ['claim_id', 'claim_text']\n",
        "test_claims_text = test_claims['claim_text']\n",
        "test_claims_id = test_claims['claim_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV2PK7ldyQao"
      },
      "outputs": [],
      "source": [
        "dev_claim_ids = dev_claims['claim_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6eWAVRHbYL_j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "dev_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/reranked_indices.json\", \"r\"))\n",
        "test_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/test_reranked_indices.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J12VT62YYjhd"
      },
      "outputs": [],
      "source": [
        "dev_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in dev_evidence_indices]\n",
        "test_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in test_evidence_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t9hCn0g-7Ae2"
      },
      "outputs": [],
      "source": [
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text.lower())\n",
        "    return tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_processed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_4DV847328f"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=3):\n",
        "    # Count all the words\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Start vocab from special tokens\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    })\n",
        "    index = 4  # Start indexing from 4 because 0-3 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:  # Only include words that meet the frequency threshold\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary using only evidence texts and applying the frequency threshold\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "99CY84id6ZJN"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    \"REFUTES\": 0,\n",
        "    \"SUPPORTS\": 1,\n",
        "    \"NOT_ENOUGH_INFO\": 2,\n",
        "    \"DISPUTED\": 3\n",
        "}\n",
        "train_claim_labels = train_claims['claim_label'].map(label_map)\n",
        "dev_claim_labels = dev_claims['claim_label'].map(label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "viPwRuD789ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text]\n",
        "\n",
        "class BinaryClaimEvidenceDataset(Dataset):\n",
        "    def __init__(self, claims, evidence_indices, evidences, claim_labels, vocab):\n",
        "        self.claims = claims\n",
        "        self.evidence_indices = evidence_indices\n",
        "        self.evidences = evidences\n",
        "        self.claim_labels = claim_labels\n",
        "        self.vocab = vocab\n",
        "        self.pairs = self.create_pairs()\n",
        "\n",
        "    def create_pairs(self):\n",
        "        pairs = []\n",
        "        for idx, claim in enumerate(self.claims):\n",
        "            label = self.claim_labels[idx]\n",
        "            if label in [0, 1]:  # Only consider REFUTES (0) and SUPPORTS (1)\n",
        "                candidate_pos_indices = self.evidence_indices[idx]\n",
        "                positive_evidences = [self.evidences[i] for i in candidate_pos_indices]\n",
        "                for evidence in positive_evidences:\n",
        "                    pairs.append((claim, evidence, label))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim, evidence, label = self.pairs[idx]\n",
        "        claim_indices = text_to_indices(claim, self.vocab)\n",
        "        evidence_indices = text_to_indices(evidence, self.vocab)\n",
        "        claim_indices = [self.vocab[\"<sos>\"]] + claim_indices + [self.vocab[\"<eos>\"]]\n",
        "        evidence_indices = [self.vocab[\"<sos>\"]] + evidence_indices + [self.vocab[\"<eos>\"]]\n",
        "        return claim_indices, evidence_indices, label\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    claims, evidences, labels = zip(*batch)\n",
        "    claims_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in claims], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "    evidences_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in evidences], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "    return claims_tensor, evidences_tensor, labels_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMmZlSPXoeM4"
      },
      "source": [
        "## 2.1 Bi-Directional GRU (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZahBwCH8BM-5"
      },
      "outputs": [],
      "source": [
        "class ClaimEvidenceBaseModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout=0.5):\n",
        "        super(ClaimEvidenceBaseModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 4, 1)  # *4 because we will concatenate hidden states of claims and evidences\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        embedded_claims = self.dropout(embedded_claims)\n",
        "        claims_mask = (claims != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_claims *= claims_mask\n",
        "\n",
        "        _, hidden_claims = self.gru(embedded_claims)\n",
        "        hidden_claims = torch.cat((hidden_claims[-2,:,:], hidden_claims[-1,:,:]), dim=1)\n",
        "\n",
        "        # Embed and encode evidences\n",
        "        embedded_evidences = self.embedding(evidences)\n",
        "        embedded_evidences = self.dropout(embedded_evidences)\n",
        "        evidences_mask = (evidences != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_evidences *= evidences_mask\n",
        "\n",
        "        _, hidden_evidences = self.gru(embedded_evidences)\n",
        "        hidden_evidences = torch.cat((hidden_evidences[-2,:,:], hidden_evidences[-1,:,:]), dim=1)\n",
        "\n",
        "        # Combine claim and evidence representations by concatenation\n",
        "        combined_representation = torch.cat((hidden_claims, hidden_evidences), dim=1)\n",
        "        combined_representation = self.dropout(combined_representation)\n",
        "        logits = self.fc(combined_representation).squeeze(-1)  # Ensure the output is of shape (batch_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Model instantiation\n",
        "model = ClaimEvidenceBaseModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=256, pad_idx=vocab['<pad>'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-smrTwUwbCO"
      },
      "source": [
        "## Bi-Directional GRU + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fq_GN0iGxO4I"
      },
      "outputs": [],
      "source": [
        "class ClaimEvidenceAttnModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout=0.5):\n",
        "        super(ClaimEvidenceAttnModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 4, 1)  # *4 because we will concatenate hidden states of claims and evidences\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "        self.attn_claims = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "        self.attn_evidences = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "    def attention(self, gru_output, attn_layer, mask):\n",
        "        attn_energies = attn_layer(gru_output).squeeze(2)  # (batch_size, seq_len)\n",
        "        attn_energies = attn_energies.masked_fill(mask.squeeze(2) == 0, -1e10)  # Apply mask\n",
        "        attn_weights = F.softmax(attn_energies, dim=1)  # (batch_size, seq_len)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), gru_output).squeeze(1)  # (batch_size, hidden_dim * 2)\n",
        "        return context\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        embedded_claims = self.dropout(embedded_claims)\n",
        "        claims_mask = (claims != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_claims *= claims_mask\n",
        "\n",
        "        gru_output_claims, _ = self.gru(embedded_claims)  # (batch_size, seq_len, hidden_dim * 2)\n",
        "        claims_context = self.attention(gru_output_claims, self.attn_claims, claims_mask)\n",
        "\n",
        "        # Embed and encode evidences\n",
        "        embedded_evidences = self.embedding(evidences)\n",
        "        embedded_evidences = self.dropout(embedded_evidences)\n",
        "        evidences_mask = (evidences != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_evidences *= evidences_mask\n",
        "\n",
        "        gru_output_evidences, _ = self.gru(embedded_evidences)  # (batch_size, seq_len, hidden_dim * 2)\n",
        "        evidences_context = self.attention(gru_output_evidences, self.attn_evidences, evidences_mask)\n",
        "\n",
        "        # Combine claim and evidence representations by concatenation\n",
        "        combined_representation = torch.cat((claims_context, evidences_context), dim=1)\n",
        "        combined_representation = self.dropout(combined_representation)\n",
        "        logits = self.fc(combined_representation).squeeze(-1)  # Ensure the output is of shape (batch_size)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFFwgHBBo69T"
      },
      "source": [
        "## 2.2 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "M8vOvSYjz3jz"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, num_epochs=10, grad_clip=1.0, threshold=0.8):\n",
        "    model = model.to(device)  # Ensure the model is on the right device\n",
        "    criterion = criterion.to(device)  # Also move the criterion to the GPU if available\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for claims, evidences, labels in train_loader:\n",
        "            claims = claims.to(device)\n",
        "            evidences = evidences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(claims, evidences).squeeze()\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Training Loss: {avg_train_loss}')\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for claims, evidences, labels in dev_loader:\n",
        "                claims = claims.to(device)\n",
        "                evidences = evidences.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                logits = model(claims, evidences).squeeze()\n",
        "                val_loss = criterion(logits, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
        "                preds = torch.where(probs > threshold, 1, 0).cpu().numpy()  # Convert probabilities to 0 and 1\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS'], zero_division=0, output_dict=True)['macro avg']['f1-score']\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {avg_val_loss}')\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(f'F1 Score: {f1}')\n",
        "        print(classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS'], zero_division=0))\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(avg_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaXEG1L00YpR"
      },
      "source": [
        "### Baseline Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UOJ_ln70WhH",
        "outputId": "310d2a21-216a-4dfa-a823-dd516ea340c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training Loss: 0.2611047856311883\n",
            "Epoch 1/15 - Validation Loss: 0.2504979223012924\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 2/15 - Training Loss: 0.24952434201156143\n",
            "Epoch 2/15 - Validation Loss: 0.241336427628994\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 3/15 - Training Loss: 0.23664563511852668\n",
            "Epoch 3/15 - Validation Loss: 0.23259469121694565\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 4/15 - Training Loss: 0.2090365663830158\n",
            "Epoch 4/15 - Validation Loss: 0.25846749544143677\n",
            "Accuracy: 0.4342105263157895\n",
            "F1 Score: 0.43289049997107765\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.30      0.96      0.46        57\n",
            "    SUPPORTS       0.96      0.26      0.41       171\n",
            "\n",
            "    accuracy                           0.43       228\n",
            "   macro avg       0.63      0.61      0.43       228\n",
            "weighted avg       0.79      0.43      0.42       228\n",
            "\n",
            "Epoch 5/15 - Training Loss: 0.17362137527856153\n",
            "Epoch 5/15 - Validation Loss: 0.24343541264533997\n",
            "Accuracy: 0.543859649122807\n",
            "F1 Score: 0.5416022270337149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.35      0.95      0.51        57\n",
            "    SUPPORTS       0.96      0.41      0.57       171\n",
            "\n",
            "    accuracy                           0.54       228\n",
            "   macro avg       0.65      0.68      0.54       228\n",
            "weighted avg       0.81      0.54      0.56       228\n",
            "\n",
            "Epoch 6/15 - Training Loss: 0.14711531762660077\n",
            "Epoch 6/15 - Validation Loss: 0.29826004803180695\n",
            "Accuracy: 0.5745614035087719\n",
            "F1 Score: 0.5654582964927792\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.36      0.86      0.50        57\n",
            "    SUPPORTS       0.91      0.48      0.63       171\n",
            "\n",
            "    accuracy                           0.57       228\n",
            "   macro avg       0.63      0.67      0.57       228\n",
            "weighted avg       0.77      0.57      0.60       228\n",
            "\n",
            "Epoch 7/15 - Training Loss: 0.1217378357145111\n",
            "Epoch 7/15 - Validation Loss: 0.32454610615968704\n",
            "Accuracy: 0.5745614035087719\n",
            "F1 Score: 0.5630544305047911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.35      0.82      0.49        57\n",
            "    SUPPORTS       0.89      0.49      0.63       171\n",
            "\n",
            "    accuracy                           0.57       228\n",
            "   macro avg       0.62      0.66      0.56       228\n",
            "weighted avg       0.76      0.57      0.60       228\n",
            "\n",
            "Epoch 8/15 - Training Loss: 0.10379959415413637\n",
            "Epoch 8/15 - Validation Loss: 0.38624827563762665\n",
            "Accuracy: 0.6622807017543859\n",
            "F1 Score: 0.6260145282576742\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.40      0.70      0.51        57\n",
            "    SUPPORTS       0.87      0.65      0.74       171\n",
            "\n",
            "    accuracy                           0.66       228\n",
            "   macro avg       0.63      0.68      0.63       228\n",
            "weighted avg       0.75      0.66      0.68       228\n",
            "\n",
            "Epoch 9/15 - Training Loss: 0.10024116312738805\n",
            "Epoch 9/15 - Validation Loss: 0.47276750206947327\n",
            "Accuracy: 0.7280701754385965\n",
            "F1 Score: 0.6664464369985843\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.47      0.60      0.52        57\n",
            "    SUPPORTS       0.85      0.77      0.81       171\n",
            "\n",
            "    accuracy                           0.73       228\n",
            "   macro avg       0.66      0.68      0.67       228\n",
            "weighted avg       0.76      0.73      0.74       228\n",
            "\n",
            "Epoch 10/15 - Training Loss: 0.08475453146131692\n",
            "Epoch 10/15 - Validation Loss: 0.40624357759952545\n",
            "Accuracy: 0.6842105263157895\n",
            "F1 Score: 0.6398736398736399\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.42      0.67      0.51        57\n",
            "    SUPPORTS       0.86      0.69      0.77       171\n",
            "\n",
            "    accuracy                           0.68       228\n",
            "   macro avg       0.64      0.68      0.64       228\n",
            "weighted avg       0.75      0.68      0.70       228\n",
            "\n",
            "Epoch 11/15 - Training Loss: 0.06981945360920071\n",
            "Epoch 11/15 - Validation Loss: 0.4386686086654663\n",
            "Accuracy: 0.6798245614035088\n",
            "F1 Score: 0.6335777029258305\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.41      0.65      0.50        57\n",
            "    SUPPORTS       0.86      0.69      0.76       171\n",
            "\n",
            "    accuracy                           0.68       228\n",
            "   macro avg       0.63      0.67      0.63       228\n",
            "weighted avg       0.74      0.68      0.70       228\n",
            "\n",
            "Epoch 12/15 - Training Loss: 0.07038388801821038\n",
            "Epoch 12/15 - Validation Loss: 0.4320005923509598\n",
            "Accuracy: 0.6622807017543859\n",
            "F1 Score: 0.6188036043860602\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.39      0.65      0.49        57\n",
            "    SUPPORTS       0.85      0.67      0.75       171\n",
            "\n",
            "    accuracy                           0.66       228\n",
            "   macro avg       0.62      0.66      0.62       228\n",
            "weighted avg       0.74      0.66      0.68       228\n",
            "\n",
            "Epoch 13/15 - Training Loss: 0.06929070459662286\n",
            "Epoch 13/15 - Validation Loss: 0.4316597878932953\n",
            "Accuracy: 0.6666666666666666\n",
            "F1 Score: 0.622483660130719\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.40      0.65      0.49        57\n",
            "    SUPPORTS       0.85      0.67      0.75       171\n",
            "\n",
            "    accuracy                           0.67       228\n",
            "   macro avg       0.62      0.66      0.62       228\n",
            "weighted avg       0.74      0.67      0.69       228\n",
            "\n",
            "Epoch 14/15 - Training Loss: 0.0676946975538029\n",
            "Epoch 14/15 - Validation Loss: 0.44382739067077637\n",
            "Accuracy: 0.6710526315789473\n",
            "F1 Score: 0.6261723105174563\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.40      0.65      0.50        57\n",
            "    SUPPORTS       0.85      0.68      0.76       171\n",
            "\n",
            "    accuracy                           0.67       228\n",
            "   macro avg       0.63      0.66      0.63       228\n",
            "weighted avg       0.74      0.67      0.69       228\n",
            "\n",
            "Epoch 15/15 - Training Loss: 0.06378885064340006\n",
            "Epoch 15/15 - Validation Loss: 0.4575580358505249\n",
            "Accuracy: 0.6710526315789473\n",
            "F1 Score: 0.6261723105174563\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.40      0.65      0.50        57\n",
            "    SUPPORTS       0.85      0.68      0.76       171\n",
            "\n",
            "    accuracy                           0.67       228\n",
            "   macro avg       0.63      0.66      0.63       228\n",
            "weighted avg       0.74      0.67      0.69       228\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_refutes = (train_claim_labels == 0).sum()\n",
        "num_supports = (train_claim_labels == 1).sum()\n",
        "pos_weight = num_refutes / (num_supports*2)\n",
        "\n",
        "train_dataset = BinaryClaimEvidenceDataset(train_claims_text_processed, train_evidence_idxs, evidence_text_processed, train_claim_labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
        "dev_dataset = BinaryClaimEvidenceDataset(dev_claims_text_processed, dev_evidence_idxs, evidence_text_processed, dev_claim_labels, vocab)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "# Training the model\n",
        "model = ClaimEvidenceBaseModel(vocab_size=len(vocab), embedding_dim=300, hidden_dim=512, pad_idx=vocab['<pad>'])\n",
        "\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, device=device, num_epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbU01oBx0g0t"
      },
      "source": [
        "### Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-oLN4jojGch",
        "outputId": "a3077945-4ce9-4ba0-9e5b-b23a7f861ab4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training Loss: 0.25629022546046604\n",
            "Epoch 1/15 - Validation Loss: 0.2503948360681534\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 2/15 - Training Loss: 0.23562038142596725\n",
            "Epoch 2/15 - Validation Loss: 0.23958025872707367\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 3/15 - Training Loss: 0.20636684355218854\n",
            "Epoch 3/15 - Validation Loss: 0.23521528393030167\n",
            "Accuracy: 0.24561403508771928\n",
            "F1 Score: 0.1971830985915493\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      0.98      0.39        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.49      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 4/15 - Training Loss: 0.17276281573339902\n",
            "Epoch 4/15 - Validation Loss: 0.25109369307756424\n",
            "Accuracy: 0.5877192982456141\n",
            "F1 Score: 0.58140625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.37      0.93      0.53        57\n",
            "    SUPPORTS       0.95      0.47      0.63       171\n",
            "\n",
            "    accuracy                           0.59       228\n",
            "   macro avg       0.66      0.70      0.58       228\n",
            "weighted avg       0.81      0.59      0.61       228\n",
            "\n",
            "Epoch 5/15 - Training Loss: 0.15063932943528732\n",
            "Epoch 5/15 - Validation Loss: 0.24539168179035187\n",
            "Accuracy: 0.5921052631578947\n",
            "F1 Score: 0.5853978061513794\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.37      0.93      0.53        57\n",
            "    SUPPORTS       0.95      0.48      0.64       171\n",
            "\n",
            "    accuracy                           0.59       228\n",
            "   macro avg       0.66      0.70      0.59       228\n",
            "weighted avg       0.81      0.59      0.61       228\n",
            "\n",
            "Epoch 6/15 - Training Loss: 0.1254473900900478\n",
            "Epoch 6/15 - Validation Loss: 0.25666461884975433\n",
            "Accuracy: 0.6359649122807017\n",
            "F1 Score: 0.6225391443103621\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.40      0.89      0.55        57\n",
            "    SUPPORTS       0.94      0.55      0.69       171\n",
            "\n",
            "    accuracy                           0.64       228\n",
            "   macro avg       0.67      0.72      0.62       228\n",
            "weighted avg       0.80      0.64      0.66       228\n",
            "\n",
            "Epoch 7/15 - Training Loss: 0.11305076783869118\n",
            "Epoch 7/15 - Validation Loss: 0.32676154375076294\n",
            "Accuracy: 0.6929824561403509\n",
            "F1 Score: 0.6684669713336102\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.44      0.84      0.58        57\n",
            "    SUPPORTS       0.92      0.64      0.76       171\n",
            "\n",
            "    accuracy                           0.69       228\n",
            "   macro avg       0.68      0.74      0.67       228\n",
            "weighted avg       0.80      0.69      0.71       228\n",
            "\n",
            "Epoch 8/15 - Training Loss: 0.09345213937020935\n",
            "Epoch 8/15 - Validation Loss: 0.47038397192955017\n",
            "Accuracy: 0.7675438596491229\n",
            "F1 Score: 0.7107914702151592\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.53      0.65      0.58        57\n",
            "    SUPPORTS       0.87      0.81      0.84       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.73      0.71       228\n",
            "weighted avg       0.79      0.77      0.77       228\n",
            "\n",
            "Epoch 9/15 - Training Loss: 0.0850084430529876\n",
            "Epoch 9/15 - Validation Loss: 0.7029775232076645\n",
            "Accuracy: 0.7807017543859649\n",
            "F1 Score: 0.7109533468559838\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.56      0.58      0.57        57\n",
            "    SUPPORTS       0.86      0.85      0.85       171\n",
            "\n",
            "    accuracy                           0.78       228\n",
            "   macro avg       0.71      0.71      0.71       228\n",
            "weighted avg       0.78      0.78      0.78       228\n",
            "\n",
            "Epoch 10/15 - Training Loss: 0.0742623793472231\n",
            "Epoch 10/15 - Validation Loss: 0.5977526903152466\n",
            "Accuracy: 0.7456140350877193\n",
            "F1 Score: 0.6819624819624819\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.49      0.60      0.54        57\n",
            "    SUPPORTS       0.86      0.80      0.82       171\n",
            "\n",
            "    accuracy                           0.75       228\n",
            "   macro avg       0.67      0.70      0.68       228\n",
            "weighted avg       0.76      0.75      0.75       228\n",
            "\n",
            "Epoch 11/15 - Training Loss: 0.06788617829108133\n",
            "Epoch 11/15 - Validation Loss: 0.6095027476549149\n",
            "Accuracy: 0.7719298245614035\n",
            "F1 Score: 0.7059523809523809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.54      0.60      0.57        57\n",
            "    SUPPORTS       0.86      0.83      0.85       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.71      0.71       228\n",
            "weighted avg       0.78      0.77      0.78       228\n",
            "\n",
            "Epoch 12/15 - Training Loss: 0.07000864804433905\n",
            "Epoch 12/15 - Validation Loss: 0.6346049606800079\n",
            "Accuracy: 0.7719298245614035\n",
            "F1 Score: 0.7027379400260756\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.54      0.58      0.56        57\n",
            "    SUPPORTS       0.86      0.84      0.85       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.71      0.70       228\n",
            "weighted avg       0.78      0.77      0.77       228\n",
            "\n",
            "Epoch 13/15 - Training Loss: 0.06704396210957021\n",
            "Epoch 13/15 - Validation Loss: 0.633887380361557\n",
            "Accuracy: 0.7719298245614035\n",
            "F1 Score: 0.7059523809523809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.54      0.60      0.57        57\n",
            "    SUPPORTS       0.86      0.83      0.85       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.71      0.71       228\n",
            "weighted avg       0.78      0.77      0.78       228\n",
            "\n",
            "Epoch 14/15 - Training Loss: 0.07174460460489566\n",
            "Epoch 14/15 - Validation Loss: 0.6302380561828613\n",
            "Accuracy: 0.7719298245614035\n",
            "F1 Score: 0.7059523809523809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.54      0.60      0.57        57\n",
            "    SUPPORTS       0.86      0.83      0.85       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.71      0.71       228\n",
            "weighted avg       0.78      0.77      0.78       228\n",
            "\n",
            "Epoch 15/15 - Training Loss: 0.06282745272877206\n",
            "Epoch 15/15 - Validation Loss: 0.6165956258773804\n",
            "Accuracy: 0.7719298245614035\n",
            "F1 Score: 0.7059523809523809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.54      0.60      0.57        57\n",
            "    SUPPORTS       0.86      0.83      0.85       171\n",
            "\n",
            "    accuracy                           0.77       228\n",
            "   macro avg       0.70      0.71      0.71       228\n",
            "weighted avg       0.78      0.77      0.78       228\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_refutes = (train_claim_labels == 0).sum()\n",
        "num_supports = (train_claim_labels == 1).sum()\n",
        "pos_weight = num_refutes / (num_supports*2)\n",
        "\n",
        "train_dataset = BinaryClaimEvidenceDataset(train_claims_text_processed, train_evidence_idxs, evidence_text_processed, train_claim_labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
        "dev_dataset = BinaryClaimEvidenceDataset(dev_claims_text_processed, dev_evidence_idxs, evidence_text_processed, dev_claim_labels, vocab)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "# Training the model\n",
        "model = ClaimEvidenceAttnModel(vocab_size=len(vocab), embedding_dim=300, hidden_dim=512, pad_idx=vocab['<pad>'])\n",
        "#model = ConcatenatedClaimEvidenceModel(vocab_size=len(vocab), embedding_dim=300, hidden_dim=512, pad_idx=vocab['<pad>'])\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, device=device, num_epochs=15, threshold=0.8)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7XcsQBMur1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZacyORUushe",
        "outputId": "eafbc9a5-9b38-42fb-8042-0ad549cb5c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4155844155844156\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        REFUTES       0.24      0.19      0.21        27\n",
            "       SUPPORTS       0.50      0.63      0.56        68\n",
            "NOT ENOUGH INFO       0.44      0.34      0.38        41\n",
            "       DISPUTED       0.13      0.11      0.12        18\n",
            "\n",
            "       accuracy                           0.42       154\n",
            "      macro avg       0.33      0.32      0.32       154\n",
            "   weighted avg       0.39      0.42      0.40       154\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, claims, evidence_idxs, evidences, claim_labels, vocab, pad_idx, device, support_threshold=0.95, refute_threshold=0.05):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_evidence_predictions = []\n",
        "    all_evidence_probs = []\n",
        "\n",
        "    for claim_tokens, evidence_idx_list, true_label in zip(claims, evidence_idxs, claim_labels):\n",
        "        # Numericalize the claim tokens\n",
        "        claim_indices = text_to_indices(claim_tokens, vocab)\n",
        "        claim_indices = [vocab[\"<sos>\"]] + claim_indices + [vocab[\"<eos>\"]]\n",
        "        claim_tensor = torch.tensor(claim_indices, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        evidence_tensors = []\n",
        "        for idx in evidence_idx_list:\n",
        "            evidence_tokens = evidences[idx]\n",
        "            evidence_indices = text_to_indices(evidence_tokens, vocab)\n",
        "            evidence_indices = [vocab[\"<sos>\"]] + evidence_indices + [vocab[\"<eos>\"]]\n",
        "            evidence_tensor = torch.tensor(evidence_indices, dtype=torch.long).to(device)\n",
        "            evidence_tensors.append(evidence_tensor)\n",
        "\n",
        "        # Pad evidence tensors to the same length\n",
        "        evidence_tensors_padded = pad_sequence(evidence_tensors, batch_first=True, padding_value=pad_idx).to(device)\n",
        "\n",
        "        evidence_predictions = []\n",
        "        evidence_probs = []\n",
        "        with torch.no_grad():\n",
        "            for evidence_tensor in evidence_tensors_padded:\n",
        "                evidence_tensor = evidence_tensor.unsqueeze(0)  # Add batch dimension\n",
        "                logits = model(claim_tensor, evidence_tensor)\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                evidence_predictions.extend(preds)\n",
        "                #prob = torch.sigmoid(logits).item()\n",
        "                #evidence_probs.append(prob)\n",
        "\n",
        "\n",
        "        aggregated_prediction = aggregate_predictions(evidence_predictions)\n",
        "        all_preds.append(aggregated_prediction)\n",
        "        all_labels.append(true_label)\n",
        "        all_evidence_predictions.append(evidence_predictions)\n",
        "        all_evidence_probs.append(evidence_probs)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'DISPUTED'], zero_division=0)\n",
        "    return accuracy, report, all_preds, all_labels, all_evidence_predictions\n",
        "\n",
        "def aggregate_predictions(evidence_predictions):\n",
        "    counter = Counter(evidence_predictions)\n",
        "    num_supports = counter[1]\n",
        "    num_refutes = counter[0]\n",
        "\n",
        "    # Handle conflicts: if both SUPPORTS and REFUTES are present\n",
        "    if num_supports > 0 and num_refutes > 0:\n",
        "        return 3  # DISPUTED if there are both SUPPORTS and REFUTES\n",
        "\n",
        "    # Determine the class if all predictions are either SUPPORTS or REFUTES (with or without NOT ENOUGH INFO)\n",
        "    if num_supports > 0 and num_refutes == 0:\n",
        "        return 1  # SUPPORTS\n",
        "\n",
        "    if num_refutes > 0 and num_supports == 0:\n",
        "        return 0  # REFUTES\n",
        "\n",
        "    # Default to NOT ENOUGH INFO if there are no SUPPORTS or REFUTES\n",
        "    return 2\n",
        "\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "accuracy, report, all_preds, all_labels, all_evidence_predictions = evaluate_model(model, dev_claims_text_processed, dev_k_indices, evidence_text_processed, dev_claim_labels, vocab, vocab[\"<pad>\"], device)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV4K6bawgyGi",
        "outputId": "61145736-cc83-4865-c148-37e6cdc08680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4155844155844156\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        REFUTES       0.31      0.15      0.20        27\n",
            "       SUPPORTS       0.52      0.72      0.60        68\n",
            "NOT ENOUGH INFO       0.23      0.27      0.25        41\n",
            "       DISPUTED       0.00      0.00      0.00        18\n",
            "\n",
            "       accuracy                           0.42       154\n",
            "      macro avg       0.27      0.28      0.26       154\n",
            "   weighted avg       0.35      0.42      0.37       154\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_model(model, claims, evidence_idxs, evidences, claim_labels, vocab, pad_idx, device, support_threshold=0.95, refute_threshold=0.05):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_evidence_predictions = []\n",
        "    all_evidence_probs = []\n",
        "\n",
        "    for claim_tokens, evidence_idx_list, true_label in zip(claims, evidence_idxs, claim_labels):\n",
        "        # Numericalize the claim tokens\n",
        "        claim_indices = text_to_indices(claim_tokens, vocab)\n",
        "        claim_indices = [vocab[\"<sos>\"]] + claim_indices + [vocab[\"<eos>\"]]\n",
        "        claim_tensor = torch.tensor(claim_indices, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        evidence_tensors = []\n",
        "        for idx in evidence_idx_list:\n",
        "            evidence_tokens = evidences[idx]\n",
        "            evidence_indices = text_to_indices(evidence_tokens, vocab)\n",
        "            evidence_indices = [vocab[\"<sos>\"]] + evidence_indices + [vocab[\"<eos>\"]]\n",
        "            evidence_tensor = torch.tensor(evidence_indices, dtype=torch.long).to(device)\n",
        "            evidence_tensors.append(evidence_tensor)\n",
        "\n",
        "        # Pad evidence tensors to the same length\n",
        "        evidence_tensors_padded = pad_sequence(evidence_tensors, batch_first=True, padding_value=pad_idx).to(device)\n",
        "\n",
        "        evidence_predictions = []\n",
        "        evidence_probs = []\n",
        "        with torch.no_grad():\n",
        "            for evidence_tensor in evidence_tensors_padded:\n",
        "                evidence_tensor = evidence_tensor.unsqueeze(0)  # Add batch dimension\n",
        "                logits = model(claim_tensor, evidence_tensor).squeeze()\n",
        "                prob = torch.sigmoid(logits).item()\n",
        "                evidence_probs.append(prob)\n",
        "                if prob > support_threshold:\n",
        "                    evidence_predictions.append(1)  # SUPPORTS\n",
        "                elif prob < refute_threshold:\n",
        "                    evidence_predictions.append(0)  # REFUTES\n",
        "                else:\n",
        "                    evidence_predictions.append(2)  # NOT ENOUGH INFO\n",
        "\n",
        "        aggregated_prediction = aggregate_predictions(evidence_predictions)\n",
        "        all_preds.append(aggregated_prediction)\n",
        "        all_labels.append(true_label)\n",
        "        all_evidence_predictions.append(evidence_predictions)\n",
        "        all_evidence_probs.append(evidence_probs)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'DISPUTED'], zero_division=0)\n",
        "    return accuracy, report, all_preds, all_labels, all_evidence_predictions\n",
        "\n",
        "def aggregate_predictions(evidence_predictions):\n",
        "    counter = Counter(evidence_predictions)\n",
        "    num_supports = counter[1]\n",
        "    num_refutes = counter[0]\n",
        "\n",
        "    # Handle conflicts: if both SUPPORTS and REFUTES are present\n",
        "    if num_supports > 0 and num_refutes > 0:\n",
        "        return 3  # DISPUTED if there are both SUPPORTS and REFUTES\n",
        "\n",
        "    # Determine the class if all predictions are either SUPPORTS or REFUTES (with or without NOT ENOUGH INFO)\n",
        "    if num_supports > 0 and num_refutes == 0:\n",
        "        return 1  # SUPPORTS\n",
        "\n",
        "    if num_refutes > 0 and num_supports == 0:\n",
        "        return 0  # REFUTES\n",
        "\n",
        "    # Default to NOT ENOUGH INFO if there are no SUPPORTS or REFUTES\n",
        "    return 2\n",
        "\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "accuracy, report, all_preds, all_labels, all_evidence_predictions = evaluate_model(model, dev_claims_text_processed, dev_k_indices, evidence_text_processed, dev_claim_labels, vocab, vocab[\"<pad>\"], device)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBDoW2xj4ITn",
        "outputId": "4068c039-111f-4011-a42e-4850903b3d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.461038961038961\n",
            "0.44155844155844154\n",
            "0.45454545454545453\n",
            "0.45454545454545453\n",
            "0.4675324675324675\n",
            "0.44805194805194803\n",
            "0.461038961038961\n",
            "0.45454545454545453\n",
            "0.461038961038961\n",
            "0.44155844155844154\n",
            "0.45454545454545453\n",
            "0.44805194805194803\n",
            "0.44805194805194803\n",
            "0.42857142857142855\n",
            "0.44155844155844154\n",
            "0.43506493506493504\n",
            "0.42857142857142855\n",
            "0.4090909090909091\n",
            "0.42207792207792205\n",
            "0.4155844155844156\n",
            "0.44805194805194803\n",
            "0.42857142857142855\n",
            "0.44155844155844154\n",
            "0.43506493506493504\n",
            "Best Support Threshold: 0.4\n",
            "Best Refute Threshold: 0.01\n",
            "Best Accuracy: 0.4675324675324675\n"
          ]
        }
      ],
      "source": [
        "def greedy_search_best_thresholds(model, claims, evidence_idxs, evidences, claim_labels, vocab, pad_idx, device):\n",
        "    best_accuracy = 0.1\n",
        "    best_support_threshold = 0.85\n",
        "    best_refute_threshold = 0.3\n",
        "\n",
        "    # Define the ranges for the thresholds\n",
        "    support_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    refute_thresholds = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "    for support_threshold in support_thresholds:\n",
        "        for refute_threshold in refute_thresholds:\n",
        "            accuracy, report, all_preds, all_labels,_ = evaluate_model(model, dev_claims_text_processed, dev_k_indices, evidence_text_processed, dev_claim_labels, vocab, vocab[\"<pad>\"], device, support_threshold=support_threshold, refute_threshold=refute_threshold)\n",
        "            print(accuracy)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_support_threshold = support_threshold\n",
        "                best_refute_threshold = refute_threshold\n",
        "\n",
        "    return best_support_threshold, best_refute_threshold, best_accuracy\n",
        "\n",
        "# Usage\n",
        "best_support_threshold, best_refute_threshold, best_accuracy = greedy_search_best_thresholds(\n",
        "    model, dev_claims_text_processed, dev_evidence_idxs, evidence_text_processed, dev_claim_labels, vocab, vocab[\"<pad>\"], device\n",
        ")\n",
        "\n",
        "print(f\"Best Support Threshold: {best_support_threshold}\")\n",
        "print(f\"Best Refute Threshold: {best_refute_threshold}\")\n",
        "print(f\"Best Accuracy: {best_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
