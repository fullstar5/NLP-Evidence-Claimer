{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksInulbb9Wun"
      },
      "source": [
        "## 1.1 Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rgvgk9W9Wun",
        "outputId": "fa0ebf8c-1ff0-48e1-d5a1-2ec364b5e40c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import random\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "6b7aa3b6-0d35-4428-ac5f-409fa84746bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IrTnFcd9Wuo"
      },
      "source": [
        "## 1.2 Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "82z9DJ119Wup"
      },
      "outputs": [],
      "source": [
        "#text preprocessing\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text.lower())\n",
        "\n",
        "    processed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.lower()\n",
        "        if token not in stopwords and token.isalpha():\n",
        "            stemmed_token = stemmer.stem(token)\n",
        "            processed_tokens.append(stemmed_token)\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_precessed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aw0txwt9Wup"
      },
      "source": [
        "## 1.3 BM25 Scores (For Negative sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rHLO51oC9Wup"
      },
      "outputs": [],
      "source": [
        "# Build inverted index\n",
        "def build_inverted_index(documents):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for i, doc in enumerate(documents):\n",
        "        term_freq = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_freq[term] += 1\n",
        "        for term, freq in term_freq.items():\n",
        "            inverted_index[term].append((i, freq))\n",
        "    return inverted_index\n",
        "\n",
        "# Compute IDF values\n",
        "def compute_idf_values(inverted_index, total_documents):\n",
        "    idf_values = {}\n",
        "    for term, postings in inverted_index.items():\n",
        "        idf_values[term] = np.log((total_documents + 1) / (len(postings) + 1))\n",
        "    return idf_values\n",
        "\n",
        "# Calculate average document length\n",
        "def calculate_avg_doc_length(documents):\n",
        "    total_length = sum(len(doc) for doc in documents)\n",
        "    return total_length / len(documents)\n",
        "#k.1.0, b = 0.78 recall=0.14\n",
        "# Compute BM25 scores\n",
        "def bm25_scores(query, inverted_index, idf_values, avg_doc_length, k1=0.4, b=0.9):\n",
        "    scores = defaultdict(float)\n",
        "    for term in query:\n",
        "        if term not in inverted_index:\n",
        "            continue\n",
        "        doc_list = inverted_index[term]\n",
        "        idf = idf_values[term]\n",
        "        for doc_id, tf in doc_list:\n",
        "            # Compute BM25 score for this document\n",
        "            doc_length = len(evidence_text_processed[doc_id])\n",
        "            numerator = idf * tf * (k1 + 1)\n",
        "            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))\n",
        "            scores[doc_id] += numerator / denominator\n",
        "    return scores\n",
        "\n",
        "# Build inverted index for evidence text\n",
        "inverted_index = build_inverted_index(evidence_text_processed)\n",
        "total_documents = len(evidence_text_processed)\n",
        "idf_values = compute_idf_values(inverted_index, total_documents)\n",
        "avg_doc_length = calculate_avg_doc_length(evidence_text_processed)\n",
        "\n",
        "# Example usage\n",
        "train_bm25_results = []\n",
        "for query in train_claims_text_processed:\n",
        "    scores = bm25_scores(query, inverted_index, idf_values, avg_doc_length)\n",
        "    train_bm25_results.append(scores)\n",
        "\n",
        "dev_bm25_results = []\n",
        "for query in dev_claims_text_precessed:\n",
        "    scores = bm25_scores(query, inverted_index, idf_values, avg_doc_length)\n",
        "    dev_bm25_results.append(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XJNfW9SJ9Wup"
      },
      "outputs": [],
      "source": [
        "train_reranked_indices = [[doc_id for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in train_bm25_results]\n",
        "train_reranked_scores = [[score for _, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in train_bm25_results]\n",
        "\n",
        "dev_reranked_indices = [[doc_id for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in dev_bm25_results]\n",
        "dev_reranked_scores = [[score for _, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in dev_bm25_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vqogmgp_GucT"
      },
      "outputs": [],
      "source": [
        "def topk_indices(indices, k=100):\n",
        "    return [indices[i][:min(k, len(indices[i]))] for i in range(len(indices))]\n",
        "\n",
        "train_top_indices = topk_indices(train_reranked_indices, k=500)\n",
        "dev_top_indices = topk_indices(dev_reranked_indices, k=50)\n",
        "dev_top_scores = topk_indices(dev_reranked_scores, k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRNKezPQ9Wup",
        "outputId": "ef5cce4a-5393-4fa5-83eb-a089b473a0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Recall: 0.4452380952380952\n"
          ]
        }
      ],
      "source": [
        "def calculate_average_recall(top_k_indices, true_indices):\n",
        "\n",
        "    recall_values = []\n",
        "\n",
        "    # Iterate over each pair of top_k_indices and corresponding true indices\n",
        "    for top_indices, true_inds in zip(top_k_indices, true_indices):\n",
        "        # Calculate the number of true positives\n",
        "        true_positives = len(set(top_indices) & set(true_inds))\n",
        "\n",
        "        # Calculate recall for this claim\n",
        "        recall = true_positives / len(true_inds) if true_inds else 0  # Avoid division by zero\n",
        "\n",
        "        # Append the recall for this claim to the list\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    # Compute average recall over all claims\n",
        "    avg_recall = sum(recall_values) / len(recall_values) if recall_values else 0  # Avoid division by zero if list is empty\n",
        "\n",
        "    return avg_recall\n",
        "\n",
        "avg_recall = calculate_average_recall(dev_top_indices, dev_evidence_idxs)\n",
        "print(\"Average Recall:\", avg_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UiGozsx3hMON"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=3):\n",
        "    # Count all the words\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Start vocab from special tokens\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    })\n",
        "    index = 4  # Start indexing from 4 because 0-3 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:  # Only include words that meet the frequency threshold\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary using only evidence texts and applying the frequency threshold\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pk8RmbV9Wuq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcWMPVTU9Wuq"
      },
      "source": [
        "## 1.4 Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG9sPShme3Na"
      },
      "source": [
        "### 1.4.1 Random Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ru_9UMnr9Wuq"
      },
      "outputs": [],
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
        "\n",
        "class RankingDatasetRandom(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices, top_k_indices, k=100, neg_samples = 32):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "        self.top_k_indices = top_k_indices\n",
        "        self.k = k\n",
        "        self.neg_samples = neg_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim = self.claims[idx]\n",
        "        true_idxs = self.true_indices[idx]\n",
        "        top_k_indices = self.top_k_indices[idx][:self.k]\n",
        "        valid_indices = [i for i in true_idxs if i in top_k_indices]\n",
        "        if not valid_indices:\n",
        "            return None\n",
        "\n",
        "        pos_idx = random.choice(valid_indices)\n",
        "        pos_evidence = self.evidences[pos_idx]\n",
        "\n",
        "        neg_indices = [ i for i in top_k_indices if i not in valid_indices]\n",
        "        neg_evidences = random.sample([self.evidences[neg_idx] for neg_idx in neg_indices], min(self.neg_samples, len(neg_indices))) # Ensure we do not exceed available negatives\n",
        "\n",
        "        return claim, pos_evidence, neg_evidences\n",
        "\n",
        "\n",
        "def random_collate_fn(batch):\n",
        "    # Remove None items that were skipped in the dataset\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if not batch:\n",
        "        # If all items are None, return None. This needs to be handled in the training loop.\n",
        "        return None\n",
        "\n",
        "    claims, pos_evidences, neg_evidences_lists = zip(*batch)\n",
        "\n",
        "    # Convert claims and evidences to indices\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    pos_indices = [text_to_indices(evidence, vocab) for evidence in pos_evidences]\n",
        "    neg_indices = [text_to_indices(neg, vocab) for sublist in neg_evidences_lists for neg in sublist]\n",
        "\n",
        "    # Pad all sequences\n",
        "    claims_padded = pad_sequence([torch.tensor(ci, dtype=torch.long) for ci in claims_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    pos_padded = pad_sequence([torch.tensor(pi, dtype=torch.long) for pi in pos_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    neg_padded = pad_sequence([torch.tensor(ni, dtype=torch.long) for ni in neg_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Now that neg_padded is fully defined, you can reshape it\n",
        "    if neg_padded.numel() > 0:  # Check to make sure there are elements to avoid size mismatch\n",
        "        neg_padded = neg_padded.view(len(batch), -1, neg_padded.size(1))\n",
        "\n",
        "    return claims_padded, pos_padded, neg_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsBNvCufgYZ"
      },
      "source": [
        "### 1.4.2 In-Batch Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZAd0TM9zf18z"
      },
      "outputs": [],
      "source": [
        "class RankingDatasetInBatch(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pos_idx = random.choice(self.true_indices[idx])  # Randomly sample one positive evidence index\n",
        "        claim = self.claims[idx]\n",
        "        pos_evidence = self.evidences[pos_idx]\n",
        "        return claim, pos_evidence\n",
        "\n",
        "def inbatch_collate_fn(batch):\n",
        "    claims, pos_evidences = zip(*batch)\n",
        "\n",
        "    # Prepare claims and positive evidences\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    pos_indices = [text_to_indices(evidence, vocab) for evidence in pos_evidences]\n",
        "\n",
        "    # Pad sequences for claims and positive evidences\n",
        "    claims_padded = pad_sequence([torch.tensor(ci, dtype=torch.long) for ci in claims_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    pos_padded = pad_sequence([torch.tensor(pi, dtype=torch.long) for pi in pos_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Generate in-batch negatives: Each claim gets the positive samples of all other claims as its negatives.\n",
        "    neg_padded_list = []\n",
        "    max_length = max([len(pi) for pi in pos_indices])  # Find the maximum length of positive evidences in the batch\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        neg_samples = [pos_indices[j] for j in range(len(batch)) if i != j]\n",
        "\n",
        "        # Pad each negative sample to the maximum length\n",
        "        neg_padded = [F.pad(torch.tensor(ni, dtype=torch.long), (0, max_length - len(ni)), value=vocab['<pad>']) for ni in neg_samples]\n",
        "        neg_padded_stack = torch.stack(neg_padded, dim=0)\n",
        "        neg_padded_list.append(neg_padded_stack)\n",
        "\n",
        "    # Stack the list of negative batches to form a single tensor\n",
        "    neg_padded_stack = torch.stack(neg_padded_list, dim=0)\n",
        "\n",
        "    return claims_padded, pos_padded, neg_padded_stack\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3pFas7fl38"
      },
      "source": [
        "### 1.4.3 In-Batch + Top Negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "95zAO4lBf2iP"
      },
      "outputs": [],
      "source": [
        "class RankingDatasetInBatchGold(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices, top_indices):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "        self.top_indices = top_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pos_idx = random.choice(self.true_indices[idx])  # Randomly sample one positive evidence index\n",
        "        claim = self.claims[idx]\n",
        "        pos_evidence = self.evidences[pos_idx]\n",
        "\n",
        "        # Sample a negative from the top indices\n",
        "        top_neg_indices = [i for i in self.top_indices[idx] if i not in self.true_indices[idx]]\n",
        "        neg_idx = random.choice(top_neg_indices[:20])  # Choose one from the top 20 indices that are not true indices\n",
        "        neg_evidence = self.evidences[neg_idx]\n",
        "\n",
        "        return claim, pos_evidence, neg_evidence\n",
        "\n",
        "def inbatch_gold_collate_fn(batch):\n",
        "    claims, pos_evidences, neg_evidences = zip(*batch)\n",
        "\n",
        "    # Prepare claims, positive evidences, and sampled negative evidences\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    pos_indices = [text_to_indices(evidence, vocab) for evidence in pos_evidences]\n",
        "    neg_indices = [text_to_indices(evidence, vocab) for evidence in neg_evidences]\n",
        "\n",
        "    # Pad sequences for claims, positive evidences, and sampled negative evidences\n",
        "    claims_padded = pad_sequence([torch.tensor(ci, dtype=torch.long) for ci in claims_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    pos_padded = pad_sequence([torch.tensor(pi, dtype=torch.long) for pi in pos_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    neg_padded = pad_sequence([torch.tensor(ni, dtype=torch.long) for ni in neg_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Generate in-batch negatives: Each claim gets the positive samples of all other claims as its negatives.\n",
        "    neg_padded_list = []\n",
        "    for i in range(len(batch)):\n",
        "        neg_samples = [pos_indices[j] for j in range(len(batch)) if i != j]\n",
        "        neg_samples.append(neg_indices[i])  # Add the sampled negative evidence for this claim\n",
        "        neg_padded_stack = pad_sequence([torch.tensor(ni, dtype=torch.long) for ni in neg_samples], batch_first=True, padding_value=vocab['<pad>'])\n",
        "        neg_padded_list.append(neg_padded_stack)\n",
        "\n",
        "    # Find the maximum length of the negative sequences in the neg_padded_list\n",
        "    max_length = max([neg.size(1) for neg in neg_padded_list])\n",
        "\n",
        "    # Pad each sequence in the neg_padded_list to the maximum length\n",
        "    neg_padded_list = [F.pad(neg, (0, max_length - neg.size(1)), value=vocab['<pad>']) for neg in neg_padded_list]\n",
        "\n",
        "    # Stack the list of negative batches to form a single tensor\n",
        "    combined_neg_padded_stack = torch.stack(neg_padded_list, dim=0)\n",
        "\n",
        "    return claims_padded, pos_padded, combined_neg_padded_stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55wRdIAo9Wuq"
      },
      "source": [
        "## 2.1 GRU (SiameseNetwork)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "class GRU_SiameseNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Initialize embedding layer with random weights\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, claim, evidence):\n",
        "        # Embed and process claim\n",
        "        claim_emb = self.embedding(claim)\n",
        "        _, claim_hidden = self.gru(claim_emb)\n",
        "        claim_hidden = claim_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "        # Embed and process evidence\n",
        "        evidence_emb = self.embedding(evidence)\n",
        "        _, evidence_hidden = self.gru(evidence_emb)\n",
        "        evidence_hidden = evidence_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "        # Calculate dot product\n",
        "        scores_dot = torch.bmm(claim_hidden.unsqueeze(1), evidence_hidden.unsqueeze(2)).squeeze()\n",
        "        return scores_dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COatKzYv9Wur"
      },
      "source": [
        "## 2.2 GRU + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9YB6O-sb9Wur"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)  # Adjusted for unidirectional GRU\n",
        "\n",
        "    def forward(self, outputs, mask):\n",
        "        attn_weights = torch.tanh(self.attn(outputs))\n",
        "        attn_weights = attn_weights.masked_fill(mask == 0, -1e9)  # Apply mask\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "        context = (attn_weights * outputs).sum(dim=1)\n",
        "        return context\n",
        "\n",
        "class GRU_Attn_SiameseNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  \n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Single layer unidirectional GRU\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=0)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attention = Attention(hidden_dim)  # Adjusted for unidirectional output\n",
        "\n",
        "    def forward_one(self, text):\n",
        "        text_emb = self.embedding(text)\n",
        "        text_out, _ = self.gru(text_emb)\n",
        "        text_out = self.dropout(text_out)\n",
        "\n",
        "        # Create mask\n",
        "        mask = (text != 0).unsqueeze(2).to(text.device) \n",
        "        text_context = self.attention(text_out, mask)\n",
        "        return text_context\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        claim_contexts = self.forward_one(claims)\n",
        "        flattened_evidences = evidences.view(-1, evidences.size(-1))\n",
        "        evidence_contexts = self.forward_one(flattened_evidences)\n",
        "        evidence_contexts = evidence_contexts.view(claims.size(0), -1, self.hidden_dim)\n",
        "\n",
        "        # Calculate dot product\n",
        "        similarities = torch.bmm(claim_contexts.unsqueeze(1), evidence_contexts.transpose(1, 2)).squeeze(1)\n",
        "        return similarities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7YMtrx09Wur"
      },
      "source": [
        "## 2.3 Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dHun-ZWjEf0h"
      },
      "outputs": [],
      "source": [
        "def listwise_loss(model, claims_emb, pos_evidences_emb, neg_evidences_emb):\n",
        "    pos_scores = model(claims_emb, pos_evidences_emb).unsqueeze(1)\n",
        "    neg_scores = torch.stack([model(claims_emb, neg) for neg in neg_evidences_emb.transpose(0, 1)], dim=1)\n",
        "\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim=1)\n",
        "    scores = scores.squeeze(-1)\n",
        "    scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "    # Create target tensor where the index of positive examples is always 0\n",
        "    target = torch.zeros(scores.size(0), dtype=torch.long, device=scores.device)\n",
        "\n",
        "    return F.nll_loss(scores, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uvGJka2u9Wur"
      },
      "outputs": [],
      "source": [
        "def margin_ranking_loss(model, claims, pos_evidences, neg_evidences, margin=1.5):\n",
        "    batch_size = claims.size(0)\n",
        "\n",
        "    # Get the scores for positive evidence\n",
        "    pos_scores = model(claims, pos_evidences).unsqueeze(1)\n",
        "\n",
        "    # Get the scores for negative evidence\n",
        "    neg_scores_list = [model(claims, neg_evidences[:, i, :]).unsqueeze(1) for i in range(neg_evidences.shape[1])]\n",
        "    neg_scores = torch.cat(neg_scores_list, dim=1)\n",
        "\n",
        "    # Calculate the margin ranking loss\n",
        "    target = torch.ones_like(neg_scores, device=claims.device)\n",
        "    pos_scores = pos_scores.expand_as(neg_scores)  # Expand pos_scores to match neg_scores shape\n",
        "    loss = F.margin_ranking_loss(pos_scores, neg_scores, target, margin=margin)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK4tCUFy9Wur"
      },
      "source": [
        "## 2.4 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pf6eC5f-9Wur"
      },
      "outputs": [],
      "source": [
        "def topk_indices(indices, k=100):\n",
        "    return [indices[i][:min(k, len(indices[i]))] for i in range(len(indices))]\n",
        "\n",
        "def evaluate_model(model, claims, evidence_texts, dev_top_indices, top_k, vocab, pad_idx):\n",
        "    # Convert tensors to lists of indices and get the top k indices for evaluation\n",
        "    dev_top_indices = topk_indices(dev_top_indices, k=top_k)\n",
        "\n",
        "    dev_scores = []\n",
        "    for idx in range(len(claims)):\n",
        "        top_k_evidence_idxs = dev_top_indices[idx]\n",
        "        top_k_evidences = [evidence_texts[i] for i in top_k_evidence_idxs]\n",
        "        scores = score_query(model, claims[idx], top_k_evidences, vocab, pad_idx)\n",
        "        dev_scores.append(scores)\n",
        "\n",
        "    reranked_indices = []\n",
        "    for indices, scores in zip(dev_top_indices, dev_scores):\n",
        "        indexed_scores = list(zip(indices, scores))\n",
        "        sorted_by_score = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
        "        sorted_indices = [idx for idx, _ in sorted_by_score]\n",
        "        reranked_indices.append(sorted_indices)\n",
        "\n",
        "    return reranked_indices\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, eval_fn, eval_data, vocab, pad_idx, topk=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"\\n\" + \"#\" * 50)  # Print separation line\n",
        "        print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            claim, pos_evidences, neg_evidences = batch\n",
        "            claim = claim.to(device)\n",
        "            pos_evidences = pos_evidences.to(device)\n",
        "            neg_evidences = neg_evidences.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model, claim, pos_evidences, neg_evidences)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss}')\n",
        "\n",
        "        scheduler.step(avg_epoch_loss)\n",
        "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "        # Evaluation at the end of each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            reranked_indices = eval_fn(model, eval_data['claims'], eval_data['evidences'], eval_data['top_indices'], top_k=topk, vocab=vocab, pad_idx=pad_idx)\n",
        "            results = evaluate_evidence_retrieval(reranked_indices, eval_data['ground_truth'], k=5)\n",
        "            print(f\"Epoch {epoch+1} Evaluation - Recall: {results['average_recall']}, Precision: {results['average_precision']}, F1 Score: {results['average_fscore']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B311yWnNbaV0"
      },
      "source": [
        "### 2.4.1 GRU + Listwise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqsSm5qIoYbP"
      },
      "source": [
        "#### Random negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Fxncv4XsZW",
        "outputId": "c58ab318-959d-42d0-9ec1-c8135e974b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 3.8578842053046594\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.11331168831168827, Precision: 0.0727272727272727, F1 Score: 0.08303442589156872\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 2.550181839710627\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.1269480519480519, Precision: 0.07792207792207788, F1 Score: 0.09074417645846213\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 1.6366371332536427\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1291125541125541, Precision: 0.07662337662337661, F1 Score: 0.08979076479076478\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 1.4144377156616132\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.15151515151515146, Precision: 0.08441558441558437, F1 Score: 0.10108740465883324\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 1.204746376787437\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.14004329004329, Precision: 0.08441558441558437, F1 Score: 0.09860853432282\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#data loading\n",
        "dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpDYcPwooeh8"
      },
      "source": [
        "#### In Batch Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1O80nHyoi52",
        "outputId": "453af2b3-ff70-4cc5-be81-0438c06dd837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 3.658553355779403\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.15086580086580081, Precision: 0.08961038961038958, F1 Score: 0.10461245104102247\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 3.3098059862087936\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.16147186147186138, Precision: 0.09999999999999994, F1 Score: 0.11586270871985152\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 2.899447141549526\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1553030303030302, Precision: 0.09350649350649344, F1 Score: 0.10856009070294782\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 2.5034423302381468\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.15681818181818172, Precision: 0.09220779220779213, F1 Score: 0.10814265099979382\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 2.059889631393628\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.1667748917748917, Precision: 0.09610389610389602, F1 Score: 0.11322923108637392\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#data loading\n",
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASUtAb3iojUn"
      },
      "source": [
        "#### In Batch + Gold Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFmU3E8zoof7",
        "outputId": "1823ab54-92a7-48b6-e287-05f09e2b977d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 3.805561206279657\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.15746753246753234, Precision: 0.09610389610389605, F1 Score: 0.11172954030096885\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 3.3852939361181016\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.14761904761904757, Precision: 0.08961038961038957, F1 Score: 0.10417439703153987\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 3.1411674878536124\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1494588744588744, Precision: 0.08961038961038956, F1 Score: 0.10449391877963306\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 2.8021051853131027\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.15443722943722937, Precision: 0.09090909090909084, F1 Score: 0.10699855699855698\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 2.4199814735314784\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.16450216450216443, Precision: 0.08961038961038953, F1 Score: 0.10703978561121416\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 2.2110763849356236\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 6 Evaluation - Recall: 0.15692640692640683, Precision: 0.09090909090909086, F1 Score: 0.10722016079158934\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 1.9955883637452736\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 7 Evaluation - Recall: 0.1653679653679653, Precision: 0.09740259740259734, F1 Score: 0.1142702535559678\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 1.7262062476231501\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 8 Evaluation - Recall: 0.1679653679653679, Precision: 0.09999999999999992, F1 Score: 0.11686765615337039\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 1.5791758512839293\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 9 Evaluation - Recall: 0.1629870129870129, Precision: 0.09740259740259735, F1 Score: 0.11389919604205312\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 1.4546628105334747\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 10 Evaluation - Recall: 0.17976190476190468, Precision: 0.1038961038961038, F1 Score: 0.12322716965574104\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#data loading\n",
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "dataset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ympctV7vbmxM"
      },
      "source": [
        "### 2.4.2 GRU + MarginRanking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjQrBlYpmCm"
      },
      "source": [
        "#### Random Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQl2eHebt5Y",
        "outputId": "61bba92a-4ff2-4bab-9d41-95705d290af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 1.950166907065954\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.14372294372294364, Precision: 0.08701298701298696, F1 Score: 0.10143784786641928\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 1.7192371710179708\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.13712121212121206, Precision: 0.08051948051948046, F1 Score: 0.09432075860647286\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 1.1022015505303175\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.12846320346320342, Precision: 0.074025974025974, F1 Score: 0.08679653679653679\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 0.42140896373595566\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.13441558441558438, Precision: 0.08441558441558439, F1 Score: 0.09736136878994023\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 0.4205237730459955\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.14177489177489172, Precision: 0.0831168831168831, F1 Score: 0.0980983302411874\n"
          ]
        }
      ],
      "source": [
        "dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dataset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rGHZujRqXMk"
      },
      "source": [
        "#### In-Batch Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Enah7CAqZv2",
        "outputId": "bebcafc0-5f93-428f-a273-1abc4971b8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 1.6285589566597571\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.14242424242424237, Precision: 0.08701298701298696, F1 Score: 0.10181405895691607\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 1.110748780079377\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.1417748917748917, Precision: 0.08571428571428565, F1 Score: 0.0998505462791177\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 0.8242228168707627\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1383116883116883, Precision: 0.08311688311688309, F1 Score: 0.0969284683570398\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 0.5456625077968988\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.14642857142857135, Precision: 0.08831168831168827, F1 Score: 0.1031127602556174\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 0.39087256445334506\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.16872294372294364, Precision: 0.09350649350649344, F1 Score: 0.11203360131931558\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dateset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y67d0B1Nqi1B"
      },
      "source": [
        "#### In-Batch Negatives + Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9nadmMnqiQd",
        "outputId": "5d67010b-ca21-4dac-ca0a-e4b7d5010bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 1.8205478986104329\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.13961038961038957, Precision: 0.0753246753246753, F1 Score: 0.08964646464646467\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 1.5460639611268654\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.14231601731601728, Precision: 0.08311688311688306, F1 Score: 0.09732529375386517\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 1.3267698257397382\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1445887445887446, Precision: 0.0883116883116883, F1 Score: 0.10235518449804165\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 1.024223186266728\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.14458874458874454, Precision: 0.09090909090909086, F1 Score: 0.10493197278911562\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 0.8217544769629453\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.14318181818181816, Precision: 0.08571428571428566, F1 Score: 0.10009791795506079\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "dataset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcZKgPuLcLy2"
      },
      "source": [
        "### 2.4.3 GRU_ATTENTION + Listwise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfSyZbYrBOq"
      },
      "source": [
        "#### Random Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udQTrAj5cTlk",
        "outputId": "8d03f297-766b-4e1e-f032-43c9bec9440d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/5\n",
            "Epoch 1/5, Average Loss: 3.622001446210421\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.171103896103896, Precision: 0.09090909090909081, F1 Score: 0.11026592455163879\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/5\n",
            "Epoch 2/5, Average Loss: 2.90422440186525\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.18030303030303027, Precision: 0.09610389610389602, F1 Score: 0.1159863945578231\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/5\n",
            "Epoch 3/5, Average Loss: 2.350960251612541\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.18452380952380945, Precision: 0.10129870129870122, F1 Score: 0.12157287157287153\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/5\n",
            "Epoch 4/5, Average Loss: 1.9073215753604205\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.17478354978354974, Precision: 0.09870129870129861, F1 Score: 0.11755308183879606\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/5\n",
            "Epoch 5/5, Average Loss: 1.5618891899402325\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.17348484848484844, Precision: 0.096103896103896, F1 Score: 0.11522366522366517\n"
          ]
        }
      ],
      "source": [
        "dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dateset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OFcRDGprEI8"
      },
      "source": [
        "#### In Batch Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPekmga5raIW",
        "outputId": "8c0c5958-1fed-47e0-9bde-32c59c70426b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 2.56662284105252\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.16396103896103895, Precision: 0.08701298701298694, F1 Score: 0.10606060606060605\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 1.9906803308389125\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.15367965367965364, Precision: 0.08441558441558437, F1 Score: 0.10103586889301175\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 1.6259772303776863\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1613636363636363, Precision: 0.08831168831168826, F1 Score: 0.10624613481756336\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 1.4131575868679926\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.17911255411255408, Precision: 0.09610389610389602, F1 Score: 0.11634199134199133\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 1.1534979373980792\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.1714285714285714, Precision: 0.08961038961038956, F1 Score: 0.10937950937950935\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 1.0272300304510655\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 6 Evaluation - Recall: 0.1737012987012987, Precision: 0.09740259740259735, F1 Score: 0.11616161616161612\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 0.9143431431207901\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 7 Evaluation - Recall: 0.182034632034632, Precision: 0.09870129870129864, F1 Score: 0.11908369408369406\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 0.8692324329645206\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 8 Evaluation - Recall: 0.17683982683982682, Precision: 0.09610389610389607, F1 Score: 0.11564110492681917\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 0.7661757304882392\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 9 Evaluation - Recall: 0.17640692640692637, Precision: 0.09480519480519477, F1 Score: 0.11466707895279321\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 0.6871257623036703\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 10 Evaluation - Recall: 0.1752164502164502, Precision: 0.09220779220779217, F1 Score: 0.11244588744588745\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dateset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsZmn9RrrIEF"
      },
      "source": [
        "#### In Batch + Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ-S1a9JrkWz",
        "outputId": "331b89d5-f497-416f-d884-b876f51f5404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 2.745050344711695\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.17283549783549776, Precision: 0.09350649350649341, F1 Score: 0.1126674912389198\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 2.149915328392616\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.18701298701298694, Precision: 0.09999999999999991, F1 Score: 0.12024840239125949\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 1.8773264579283886\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.17435064935064928, Precision: 0.09090909090909086, F1 Score: 0.1107039785611214\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 1.6105697078582568\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.19859307359307352, Precision: 0.1077922077922077, F1 Score: 0.1298701298701298\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 1.4054259703709528\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.1824675324675324, Precision: 0.1025974025974025, F1 Score: 0.12237682951968663\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 1.2663853978499389\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 6 Evaluation - Recall: 0.17781385281385273, Precision: 0.09999999999999992, F1 Score: 0.11898577612863324\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 1.1547582057806163\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 7 Evaluation - Recall: 0.17943722943722937, Precision: 0.09870129870129862, F1 Score: 0.11785198928056068\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 1.0953649442929487\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 8 Evaluation - Recall: 0.18430735930735923, Precision: 0.09999999999999994, F1 Score: 0.12011956297670583\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 1.0045462006177657\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 9 Evaluation - Recall: 0.17424242424242417, Precision: 0.09740259740259735, F1 Score: 0.11582663368377653\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 0.8420343032250037\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 10 Evaluation - Recall: 0.184090909090909, Precision: 0.09999999999999991, F1 Score: 0.12008348794063076\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "dataset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim, dropout_rate=0.6).to(device)\n",
        "\n",
        "criterion = listwise_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prgrt6chcWXT"
      },
      "source": [
        "### 2.4.4 GRU_ATTENTION + MarginRanking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQIPcXRmr_A3"
      },
      "source": [
        "#### Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY4JPSkhcdQS",
        "outputId": "ca08ff5f-dfce-4980-957d-020fdfdeb74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 1.4351360339384813\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 1 Evaluation - Recall: 0.13041125541125537, Precision: 0.07142857142857141, F1 Score: 0.08560090702947847\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 1.355090245222434\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 2 Evaluation - Recall: 0.1285714285714285, Precision: 0.07012987012987011, F1 Score: 0.08412183055040198\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 1.2465319618200645\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 3 Evaluation - Recall: 0.12835497835497828, Precision: 0.07012987012987011, F1 Score: 0.08408575551432694\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 1.177415933364477\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 4 Evaluation - Recall: 0.12922077922077915, Precision: 0.07012987012987011, F1 Score: 0.08441043083900228\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 1.105759689441094\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 5 Evaluation - Recall: 0.1259740259740259, Precision: 0.06883116883116883, F1 Score: 0.082555143269429\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 1.032260164236411\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 6 Evaluation - Recall: 0.12889610389610387, Precision: 0.07012987012987014, F1 Score: 0.08426613069470214\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 0.9943978220988543\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 7 Evaluation - Recall: 0.13300865800865797, Precision: 0.07142857142857142, F1 Score: 0.08644609358895074\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 0.9258909347729806\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 8 Evaluation - Recall: 0.12326839826839822, Precision: 0.06883116883116883, F1 Score: 0.08242630385487529\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 0.8484135545217074\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 9 Evaluation - Recall: 0.12391774891774889, Precision: 0.06753246753246754, F1 Score: 0.08168418882704598\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 0.790667087603838\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 10 Evaluation - Recall: 0.12651515151515147, Precision: 0.07012987012987014, F1 Score: 0.08428159142444856\n"
          ]
        }
      ],
      "source": [
        "dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dateset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxvT6Q6rsEnY"
      },
      "source": [
        "#### In batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PGQUDDsHAz",
        "outputId": "6291ec72-c58f-4596-a24b-2e09aa7a8e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 0.7513295641312232\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 1 Evaluation - Recall: 0.15054112554112545, Precision: 0.08701298701298696, F1 Score: 0.10258709544423826\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 0.6695915697476803\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 2 Evaluation - Recall: 0.1496753246753246, Precision: 0.08701298701298694, F1 Score: 0.10226242011956294\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 0.6344564740474408\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 3 Evaluation - Recall: 0.14101731601731593, Precision: 0.08441558441558436, F1 Score: 0.09847454133168414\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 0.5851351848015418\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 4 Evaluation - Recall: 0.14069264069264062, Precision: 0.08441558441558436, F1 Score: 0.098330241187384\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 0.5209561089674631\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 5 Evaluation - Recall: 0.13614718614718607, Precision: 0.08311688311688306, F1 Score: 0.09672232529375384\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 0.5163194644145477\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 6 Evaluation - Recall: 0.13354978354978347, Precision: 0.08051948051948048, F1 Score: 0.09412492269635125\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 0.47759584280160755\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 7 Evaluation - Recall: 0.1443722943722943, Precision: 0.08441558441558436, F1 Score: 0.09953617810760666\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 0.41524701775648654\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 8 Evaluation - Recall: 0.1378787878787878, Precision: 0.08311688311688306, F1 Score: 0.09737167594310449\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 0.4060404621637784\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 9 Evaluation - Recall: 0.1443722943722943, Precision: 0.08441558441558436, F1 Score: 0.09953617810760666\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 0.3841090821302854\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 10 Evaluation - Recall: 0.1391774891774891, Precision: 0.08441558441558436, F1 Score: 0.09867037724180579\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "# dateset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2N_C6PZsJ9D"
      },
      "source": [
        "#### In Batch + Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z_KZFiRsKIz",
        "outputId": "43a17962-21ee-401c-a78c-26d7d818b190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "Starting Epoch 1/10\n",
            "Epoch 1/10, Average Loss: 0.7837640406229557\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 1 Evaluation - Recall: 0.14556277056277053, Precision: 0.07662337662337658, F1 Score: 0.09297052154195012\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 2/10\n",
            "Epoch 2/10, Average Loss: 0.7222232298973279\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 2 Evaluation - Recall: 0.14880952380952378, Precision: 0.07792207792207788, F1 Score: 0.0948258091115234\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 3/10\n",
            "Epoch 3/10, Average Loss: 0.6873870491981506\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 3 Evaluation - Recall: 0.15140692640692638, Precision: 0.08051948051948046, F1 Score: 0.097423211708926\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 4/10\n",
            "Epoch 4/10, Average Loss: 0.666056194366553\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 4 Evaluation - Recall: 0.14848484848484844, Precision: 0.07922077922077916, F1 Score: 0.09571222428365285\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 5/10\n",
            "Epoch 5/10, Average Loss: 0.589661323871368\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 5 Evaluation - Recall: 0.15194805194805192, Precision: 0.08181818181818176, F1 Score: 0.09863430220573077\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 6/10\n",
            "Epoch 6/10, Average Loss: 0.5686737528214088\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 6 Evaluation - Recall: 0.15194805194805192, Precision: 0.08181818181818176, F1 Score: 0.09863430220573077\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 7/10\n",
            "Epoch 7/10, Average Loss: 0.5341432094573975\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 7 Evaluation - Recall: 0.15411255411255406, Precision: 0.08311688311688305, F1 Score: 0.1002576788291074\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 8/10\n",
            "Epoch 8/10, Average Loss: 0.5190028273142301\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 8 Evaluation - Recall: 0.15108225108225104, Precision: 0.08181818181818176, F1 Score: 0.09830962688105545\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 9/10\n",
            "Epoch 9/10, Average Loss: 0.4736219797378931\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 9 Evaluation - Recall: 0.15541125541125536, Precision: 0.08441558441558436, F1 Score: 0.1015563801278087\n",
            "\n",
            "##################################################\n",
            "Starting Epoch 10/10\n",
            "Epoch 10/10, Average Loss: 0.4609044278279329\n",
            "Current Learning Rate: 0.0001\n",
            "Epoch 10 Evaluation - Recall: 0.1545454545454545, Precision: 0.08441558441558436, F1 Score: 0.10123170480313337\n"
          ]
        }
      ],
      "source": [
        "# dataset = RankingDatasetRandom(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=random_collate_fn)\n",
        "# dataset = RankingDatasetInBatch(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_collate_fn)\n",
        "dataset = RankingDatasetInBatchGold(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=inbatch_gold_collate_fn)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "# Initialize model\n",
        "gru_model = GRU_Attn_SiameseNetwork(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = margin_ranking_loss\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "eval_data = {\n",
        "    'claims': dev_claims_text_precessed,\n",
        "    'evidences': evidence_text_processed,\n",
        "    'top_indices': dev_top_indices,\n",
        "    'ground_truth': dev_evidence_idxs\n",
        "}\n",
        "\n",
        "\n",
        "train_model(gru_model, dataloader, criterion, optimizer, scheduler, device, num_epochs, clip_value, evaluate_model, eval_data, vocab, vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rci2t8ywOOpx"
      },
      "source": [
        "## 3.1 Evaluation Function (Need execute before training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "def score_query(model, query, evidences, vocab, pad_idx):\n",
        "    # Convert query and evidences to indices using the same function as during training\n",
        "    query_indices = text_to_indices(query, vocab)  # Query to indices\n",
        "    evidence_indices = [text_to_indices(evidence, vocab) for evidence in evidences]  # Evidences to indices\n",
        "\n",
        "    # Convert lists to tensors and pad\n",
        "    query_tensor = pad_sequence([torch.tensor(query_indices)], batch_first=True, padding_value=pad_idx)\n",
        "    evidence_tensors = pad_sequence([torch.tensor(ei) for ei in evidence_indices], batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "    query_tensor = query_tensor.to(device)\n",
        "    evidence_tensors = evidence_tensors.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        # Process all evidences in one batch for efficiency\n",
        "        for i in range(evidence_tensors.shape[0]):\n",
        "            score = model(query_tensor, evidence_tensors[i].unsqueeze(0))\n",
        "            scores.append(score.item())\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "76a6ujov_Rt7"
      },
      "outputs": [],
      "source": [
        "def evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list, k=5):\n",
        "    assert len(predicted_indices_list) == len(actual_indices_list), \"Both inputs must have the same length.\"\n",
        "\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_fscore = 0.0\n",
        "    num_claims = len(predicted_indices_list)\n",
        "\n",
        "    for predicted_indices, actual_indices in zip(predicted_indices_list, actual_indices_list):\n",
        "        # Convert tensors in predicted_indices to integers if they are not already\n",
        "        predicted_indices = [index.item() if isinstance(index, torch.Tensor) else index for index in predicted_indices]\n",
        "\n",
        "        # Retrieve the top k predictions\n",
        "        top_k_predicted = set(predicted_indices[:k])\n",
        "        actual_indices_set = set(actual_indices)\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        correct_predictions = len(top_k_predicted.intersection(actual_indices_set))\n",
        "\n",
        "        # Calculate metrics\n",
        "        if correct_predictions > 0:\n",
        "            recall = float(correct_predictions) / len(actual_indices_set)\n",
        "            precision = float(correct_predictions) / k\n",
        "            if (precision + recall) != 0:\n",
        "                fscore = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                fscore = 0.0\n",
        "        else:\n",
        "            recall = 0.0\n",
        "            precision = 0.0\n",
        "            fscore = 0.0\n",
        "\n",
        "        # Accumulate the metrics to calculate averages later\n",
        "        total_recall += recall\n",
        "        total_precision += precision\n",
        "        total_fscore += fscore\n",
        "\n",
        "    # Calculate average metrics\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_fscore = total_fscore / num_claims\n",
        "\n",
        "    return {\n",
        "        \"average_recall\": average_recall,\n",
        "        \"average_precision\": average_precision,\n",
        "        \"average_fscore\": average_fscore\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOf2vsz9OaLu"
      },
      "source": [
        "### Test Result Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGSpdoIJOg4g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
