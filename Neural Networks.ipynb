{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5780bc5c-e36f-4ccc-8249-7753068da280",
   "metadata": {},
   "source": [
    "# Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b0f1a09-b925-476e-bf43-3431c954b15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d8eb8-a522-4321-a644-bc61ed056b20",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4894b672-25af-46b4-af59-307bb4bf3976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domain1 = pd.read_csv('processed_data/domain1_processed.csv')\n",
    "domain2 = pd.read_csv('processed_data/domain2_processed.csv')\n",
    "test = pd.read_csv('processed_data/test_processed.csv')\n",
    "\n",
    "# Split the data for domain1\n",
    "train_d1, val_d1 = train_test_split(domain1, train_size=0.8, random_state=88, stratify=domain1['label'])\n",
    "\n",
    "# Split the data for domain2\n",
    "train_d2, val_d2 = train_test_split(domain2, train_size=0.8, random_state=88, stratify=domain2['label'])\n",
    "\n",
    "# Combine d1 and d2 for training\n",
    "train_combined = pd.concat([train_d1, train_d2], ignore_index=True)\n",
    "val_combined = pd.concat([val_d1, val_d2], ignore_index=True)\n",
    "\n",
    "train_combined.reset_index(drop=True, inplace=True)\n",
    "val_d1.reset_index(drop=True, inplace=True)\n",
    "val_d2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "950081df-e3ca-42ac-aa84-d9f94f23c99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% of the sequences in domain1 have length <= 111.0\n",
      "95% of the sequences in domain2 have length <= 516.0\n",
      "95% of the sequences in domain2 have length <= 294.0\n"
     ]
    }
   ],
   "source": [
    "list_d1 = domain1['text'].apply(lambda x:len(x.split()))\n",
    "\n",
    "list_d2 = domain2['text'].apply(lambda x:len(x.split()))\n",
    "\n",
    "list_combined = train_combined['text'].apply(lambda x:len(x.split()))\n",
    "\n",
    "quantile_d1 = list_d1.quantile(0.95)\n",
    "\n",
    "quantile_d2 = list_d2.quantile(0.95)\n",
    "\n",
    "quantile_combined = list_combined.quantile(0.95)\n",
    "\n",
    "print(f\"95% of the sequences in domain1 have length <= {quantile_d1}\")\n",
    "\n",
    "print(f\"95% of the sequences in domain2 have length <= {quantile_d2}\")\n",
    "\n",
    "print(f\"95% of the sequences in domain2 have length <= {quantile_combined}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1b73e07e-9f37-46b3-95ee-5c8dcfbe2869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d1_class_0_count = len(train_combined[(train_combined['label'] == 0) & (train_combined['domain'] == 1)])\n",
    "d1_class_1_count = len(train_combined[(train_combined['label'] == 1) & (train_combined['domain'] == 1)])\n",
    "d2_class_0_count = len(train_combined[(train_combined['label'] == 0) & (train_combined['domain'] == 2)])\n",
    "d2_class_1_count = len(train_combined[(train_combined['label'] == 1) & (train_combined['domain'] == 2)])\n",
    "\n",
    "total_samples = len(train_combined)\n",
    "\n",
    "desired_samples_per_group = total_samples / 4\n",
    "\n",
    "weight_d1_class_0 = desired_samples_per_group / d1_class_0_count\n",
    "weight_d1_class_1 = desired_samples_per_group / d1_class_1_count\n",
    "weight_d2_class_0 = desired_samples_per_group / d2_class_0_count\n",
    "weight_d2_class_1 = desired_samples_per_group / d2_class_1_count\n",
    "\n",
    "def compute_weight(row):\n",
    "    if row['domain'] == 1 and row['label'] == 0:\n",
    "        return weight_d1_class_0\n",
    "    elif row['domain'] == 1 and row['label'] == 1:\n",
    "        return weight_d1_class_1\n",
    "    elif row['domain'] == 2 and row['label'] == 0:\n",
    "        return weight_d2_class_0\n",
    "    elif row['domain'] == 2 and row['label'] == 1:\n",
    "        return weight_d2_class_1\n",
    "    \n",
    "train_combined['weights'] = train_combined.apply(compute_weight, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75058fbc-0e06-435a-9915-c3d0332ee446",
   "metadata": {},
   "source": [
    "# Simple Feedfoward networks & LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9e146bc2-1306-4b66-bfa2-d24b915406f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "class Padded_Dataset(Dataset):\n",
    "    def __init__(self, data, labels_column='label'):\n",
    "        self.data = data\n",
    "        self.max_length = MAX_LENGTH\n",
    "        self.labels_column = labels_column\n",
    "        self.has_labels = labels_column in data.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_data = self.data.iloc[idx]['text']\n",
    "        \n",
    "        # Split the space-separated string of token IDs and convert to integers\n",
    "        token_ids = [int(token) for token in txt_data.split()]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Truncate or pad to MAX_LENGTH\n",
    "        if len(token_ids) > self.max_length:\n",
    "            token_ids = token_ids[:self.max_length]  # Truncate if longer\n",
    "        elif len(token_ids) < self.max_length:\n",
    "            token_ids.extend([0] * (self.max_length - len(token_ids)))  # Pad with zeros\n",
    "    \n",
    "        # Convert the list of token IDs to a PyTorch tensor\n",
    "        txt = torch.tensor(token_ids, dtype=torch.long)\n",
    "        \n",
    "        if self.has_labels:\n",
    "            label = torch.tensor(self.data.iloc[idx][self.labels_column])\n",
    "            return txt, label\n",
    "        else:\n",
    "            return txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de528f51-12c2-41d3-97ff-212fe174c0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Label_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, dropout_rate=0.5):\n",
    "        # Initializing the parent class, nn.Module.\n",
    "        super(Label_Classifier, self).__init__()  # Call the initialization of the superclass nn.Module.\n",
    "\n",
    "        # Embedding layer that converts token IDs into dense vectors of fixed size.\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)  # Initialize an embedding layer with given vocabulary size and embedding size.\n",
    "        \n",
    "         # Dropout layer after embedding\n",
    "        self.dropout_after_embedding = nn.Dropout(dropout_rate) \n",
    "\n",
    "        # A sequential container in which input is processed by each module (layer) in the defined order.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(emb_size, hidden_size),      \n",
    "            nn.ReLU(),                            \n",
    "            nn.Dropout(dropout_rate),              \n",
    "            nn.Linear(hidden_size, hidden_size),   \n",
    "            nn.ReLU(),                             \n",
    "            nn.Dropout(dropout_rate),              \n",
    "            nn.Linear(hidden_size, 1)              \n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Convert token IDs in 'text' into dense vectors.\n",
    "        text_emb = self.embedding(text)          \n",
    "        # Apply dropout after embedding\n",
    "        text_emb = self.dropout_after_embedding(text_emb)\n",
    "        \n",
    "        # Average the embeddings along the sequence dimension (common method to handle variable-length sequences).\n",
    "        text_emb = text_emb.mean(dim=1)          \n",
    "        \n",
    "        # Pass the averaged embeddings through the classifier to produce an output.\n",
    "        output = self.classifier(text_emb)       \n",
    "\n",
    "        return output.squeeze()                  \n",
    "    \n",
    "    \n",
    "class LSTM_Label_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers=1, dropout_prob=0.5):\n",
    "        super(LSTM_Label_Classifier, self).__init__()\n",
    "\n",
    "        # Embedding layer with dropout\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, emb_size),\n",
    "            nn.Dropout(p=dropout_prob)  # Add dropout to the embedding layer\n",
    "        )\n",
    "\n",
    "        # LSTM layer without dropout when num_layers is 1\n",
    "        if num_layers > 1:\n",
    "            self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Add dropout after ReLU\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Convert token IDs in 'text' into dense vectors with dropout.\n",
    "        text_emb = self.embedding(text)\n",
    "        \n",
    "\n",
    "        # Pass embeddings through LSTM\n",
    "        lstm_out, _ = self.lstm(text_emb)\n",
    "\n",
    "        \n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass the last hidden state through the classifier\n",
    "        output = self.classifier(last_hidden)\n",
    "\n",
    "        return output.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95313b1b-939f-4f87-9923-f5798968227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop for one epoch\n",
    "def train_one_epoch_nn(model, train_loader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0   # Keep track of correct predictions\n",
    "    total_examples = 0        # Keep track of total examples processed\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        txt, label = batch[0].to(device), batch[1].to(device).float()\n",
    "        x_o = model.forward(txt)\n",
    "        loss = criterion(x_o, label)\n",
    "        optimizer.zero_grad()   # Typo corrected from 'oprimizer' to 'optimizer'\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debugging and tracking\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.round(torch.sigmoid(x_o))\n",
    "        train_preds += predictions.tolist()\n",
    "        train_labels += label.tolist()\n",
    "        \n",
    "        # Update correct predictions and total examples\n",
    "        correct_predictions += (predictions == label).sum().item()\n",
    "        total_examples += label.size(0)  # Assuming label is of shape (batch_size,)\n",
    "        \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    train_accuracy = correct_predictions / total_examples  # Compute accuracy\n",
    "    \n",
    "    return average_loss, train_f1, train_accuracy\n",
    "    \n",
    "        \n",
    "#prediction function        \n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            txt = batch[0].to(device)\n",
    "            x_o = model(txt)\n",
    "            preds = torch.round(torch.sigmoid(x_o))\n",
    "            all_preds += preds.tolist()\n",
    "\n",
    "    return all_preds\n",
    "        \n",
    "        \n",
    "# Evaluation funtion        \n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    eval_preds = []\n",
    "    eval_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            txt, label = batch[0].to(device), batch[1].to(device).float()\n",
    "            x_o = model(txt)\n",
    "            loss = criterion(x_o, label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(x_o))\n",
    "            correct += (preds == label).sum().item()\n",
    "\n",
    "            eval_preds += preds.tolist()\n",
    "            eval_labels += label.tolist()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    val_f1 = f1_score(eval_labels, eval_preds, average='macro')\n",
    "    \n",
    "    return average_loss, val_f1, accuracy   #, all_labels, all_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4610dae-bc19-4c16-968a-37c40e547442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset for training\n",
    "batch_size = 64\n",
    "\n",
    "# Convert the train_combined to the pytorch Dataset tensor format with padding\n",
    "train_combined_db = Padded_Dataset(train_combined)\n",
    "train_loader = DataLoader(train_combined_db, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_d1_db = Padded_Dataset(train_d1)\n",
    "train_d1_loader = DataLoader(train_d1_db, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Convert the validation sets\n",
    "val_d1_db = Padded_Dataset(val_d1)\n",
    "val_d2_db = Padded_Dataset(val_d2)\n",
    "\n",
    "val_d1_loader = DataLoader(val_d1_db, batch_size=batch_size, shuffle=False)\n",
    "val_d2_loader = DataLoader(val_d2_db, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convert the test set\n",
    "test_db = Padded_Dataset(test)\n",
    "test_loader = DataLoader(test_db, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "82955a07-73ef-46e1-99b7-8289134a94f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_d1 loss: 0.5136, val_d2 loss: 0.4635, val_d1_f1: 0.7842, val_d2_f1: 0.4619, val_d1_accuracy: 0.7844, val_d2_accuracy: 0.8507\n",
      "Epoch 2, val_d1 loss: 0.3707, val_d2 loss: 0.4196, val_d1_f1: 0.8675, val_d2_f1: 0.4618, val_d1_accuracy: 0.8679, val_d2_accuracy: 0.8503\n",
      "Epoch 3, val_d1 loss: 0.3271, val_d2 loss: 0.4084, val_d1_f1: 0.8933, val_d2_f1: 0.4616, val_d1_accuracy: 0.8938, val_d2_accuracy: 0.8497\n",
      "Epoch 4, val_d1 loss: 0.2846, val_d2 loss: 0.4078, val_d1_f1: 0.8999, val_d2_f1: 0.4613, val_d1_accuracy: 0.9005, val_d2_accuracy: 0.8487\n",
      "Epoch 5, val_d1 loss: 0.2752, val_d2 loss: 0.4015, val_d1_f1: 0.9053, val_d2_f1: 0.4617, val_d1_accuracy: 0.9054, val_d2_accuracy: 0.8500\n",
      "Epoch 6, val_d1 loss: 0.2704, val_d2 loss: 0.4065, val_d1_f1: 0.9069, val_d2_f1: 0.4629, val_d1_accuracy: 0.9074, val_d2_accuracy: 0.8470\n",
      "Epoch 7, val_d1 loss: 0.2442, val_d2 loss: 0.4093, val_d1_f1: 0.9108, val_d2_f1: 0.4630, val_d1_accuracy: 0.9113, val_d2_accuracy: 0.8473\n",
      "Epoch 8, val_d1 loss: 0.2441, val_d2 loss: 0.4071, val_d1_f1: 0.9148, val_d2_f1: 0.4631, val_d1_accuracy: 0.9151, val_d2_accuracy: 0.8477\n",
      "Epoch 9, val_d1 loss: 0.2419, val_d2 loss: 0.4038, val_d1_f1: 0.9100, val_d2_f1: 0.4614, val_d1_accuracy: 0.9100, val_d2_accuracy: 0.8490\n",
      "Epoch 10, val_d1 loss: 0.2546, val_d2 loss: 0.4053, val_d1_f1: 0.9112, val_d2_f1: 0.4624, val_d1_accuracy: 0.9118, val_d2_accuracy: 0.8453\n",
      "Epoch 11, val_d1 loss: 0.2315, val_d2 loss: 0.4059, val_d1_f1: 0.9136, val_d2_f1: 0.4614, val_d1_accuracy: 0.9136, val_d2_accuracy: 0.8490\n",
      "Epoch 12, val_d1 loss: 0.2233, val_d2 loss: 0.4055, val_d1_f1: 0.9196, val_d2_f1: 0.4633, val_d1_accuracy: 0.9197, val_d2_accuracy: 0.8483\n",
      "Epoch 13, val_d1 loss: 0.2368, val_d2 loss: 0.3997, val_d1_f1: 0.9082, val_d2_f1: 0.4593, val_d1_accuracy: 0.9082, val_d2_accuracy: 0.8493\n",
      "Epoch 14, val_d1 loss: 0.2558, val_d2 loss: 0.4069, val_d1_f1: 0.9133, val_d2_f1: 0.4624, val_d1_accuracy: 0.9138, val_d2_accuracy: 0.8453\n",
      "Epoch 15, val_d1 loss: 0.2262, val_d2 loss: 0.3967, val_d1_f1: 0.9126, val_d2_f1: 0.4592, val_d1_accuracy: 0.9126, val_d2_accuracy: 0.8490\n",
      "Epoch 16, val_d1 loss: 0.2166, val_d2 loss: 0.4043, val_d1_f1: 0.9187, val_d2_f1: 0.4614, val_d1_accuracy: 0.9187, val_d2_accuracy: 0.8490\n",
      "Epoch 17, val_d1 loss: 0.2281, val_d2 loss: 0.4006, val_d1_f1: 0.9216, val_d2_f1: 0.4631, val_d1_accuracy: 0.9218, val_d2_accuracy: 0.8477\n",
      "Epoch 18, val_d1 loss: 0.2193, val_d2 loss: 0.4016, val_d1_f1: 0.9219, val_d2_f1: 0.4631, val_d1_accuracy: 0.9221, val_d2_accuracy: 0.8477\n",
      "Epoch 19, val_d1 loss: 0.2176, val_d2 loss: 0.4071, val_d1_f1: 0.9219, val_d2_f1: 0.4631, val_d1_accuracy: 0.9221, val_d2_accuracy: 0.8477\n",
      "Epoch 20, val_d1 loss: 0.2100, val_d2 loss: 0.4015, val_d1_f1: 0.9212, val_d2_f1: 0.4632, val_d1_accuracy: 0.9213, val_d2_accuracy: 0.8480\n"
     ]
    }
   ],
   "source": [
    "#Initialise the training process for the feed foward NN\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "\n",
    "ff_model=Label_Classifier(vocab_size=5000,emb_size=128,hidden_size=64).to(device)\n",
    "\n",
    "optimizer=optim.Adam(ff_model.parameters(),lr=0.001)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Training the Label_Classifier\n",
    "num_epoch=20\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, train_f1, train_accuracy = train_one_epoch_nn(ff_model, train_loader, criterion, device)\n",
    "    val_d1_loss, val_d1_f1 ,val_d1_accuracy = evaluate(ff_model, val_d1_loader, criterion, device)\n",
    "    val_d2_loss, val_d2_f1, val_d2_accuracy = evaluate(ff_model, val_d2_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}, val_d1 loss: {val_d1_loss:.4f}, val_d2 loss: {val_d2_loss:.4f}, val_d1_f1: {val_d1_f1:.4f}, val_d2_f1: {val_d2_f1:.4f}, val_d1_accuracy: {val_d1_accuracy:.4f}, val_d2_accuracy: {val_d2_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38f31281-80ed-44a0-b7d3-328c8fd6912e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_d1 loss: 0.7499, val_d2 loss: 0.5070, val_d1_f1: 0.3333, val_d2_f1: 0.4611, val_d1_accuracy: 0.5000, val_d2_accuracy: 0.8557\n",
      "Epoch 2, val_d1 loss: 0.7638, val_d2 loss: 0.4873, val_d1_f1: 0.3333, val_d2_f1: 0.4611, val_d1_accuracy: 0.5000, val_d2_accuracy: 0.8557\n",
      "Epoch 3, val_d1 loss: 0.7360, val_d2 loss: 0.5321, val_d1_f1: 0.3333, val_d2_f1: 0.4611, val_d1_accuracy: 0.5000, val_d2_accuracy: 0.8557\n",
      "Epoch 4, val_d1 loss: 0.7439, val_d2 loss: 0.5261, val_d1_f1: 0.3333, val_d2_f1: 0.4674, val_d1_accuracy: 0.5000, val_d2_accuracy: 0.8540\n",
      "Epoch 5, val_d1 loss: 0.4129, val_d2 loss: 0.4133, val_d1_f1: 0.8281, val_d2_f1: 0.4613, val_d1_accuracy: 0.8290, val_d2_accuracy: 0.8487\n",
      "Epoch 6, val_d1 loss: 0.3485, val_d2 loss: 0.4208, val_d1_f1: 0.8673, val_d2_f1: 0.4596, val_d1_accuracy: 0.8674, val_d2_accuracy: 0.8503\n",
      "Epoch 7, val_d1 loss: 0.3607, val_d2 loss: 0.4203, val_d1_f1: 0.8593, val_d2_f1: 0.4597, val_d1_accuracy: 0.8597, val_d2_accuracy: 0.8507\n",
      "Epoch 8, val_d1 loss: 0.3229, val_d2 loss: 0.4205, val_d1_f1: 0.8775, val_d2_f1: 0.4640, val_d1_accuracy: 0.8777, val_d2_accuracy: 0.8503\n",
      "Epoch 9, val_d1 loss: 0.3144, val_d2 loss: 0.4155, val_d1_f1: 0.8728, val_d2_f1: 0.4621, val_d1_accuracy: 0.8731, val_d2_accuracy: 0.8513\n",
      "Epoch 10, val_d1 loss: 0.3434, val_d2 loss: 0.4484, val_d1_f1: 0.8933, val_d2_f1: 0.4622, val_d1_accuracy: 0.8933, val_d2_accuracy: 0.8517\n",
      "Epoch 11, val_d1 loss: 0.2944, val_d2 loss: 0.4289, val_d1_f1: 0.8813, val_d2_f1: 0.4597, val_d1_accuracy: 0.8815, val_d2_accuracy: 0.8507\n",
      "Epoch 12, val_d1 loss: 0.2901, val_d2 loss: 0.4407, val_d1_f1: 0.8806, val_d2_f1: 0.4621, val_d1_accuracy: 0.8810, val_d2_accuracy: 0.8513\n",
      "Epoch 13, val_d1 loss: 0.3644, val_d2 loss: 0.4197, val_d1_f1: 0.8166, val_d2_f1: 0.4604, val_d1_accuracy: 0.8203, val_d2_accuracy: 0.8534\n",
      "Epoch 14, val_d1 loss: 0.2856, val_d2 loss: 0.4338, val_d1_f1: 0.8749, val_d2_f1: 0.4761, val_d1_accuracy: 0.8756, val_d2_accuracy: 0.8487\n",
      "Epoch 15, val_d1 loss: 0.2933, val_d2 loss: 0.4325, val_d1_f1: 0.8815, val_d2_f1: 0.4604, val_d1_accuracy: 0.8821, val_d2_accuracy: 0.8534\n",
      "Epoch 16, val_d1 loss: 0.2554, val_d2 loss: 0.4266, val_d1_f1: 0.8993, val_d2_f1: 0.4647, val_d1_accuracy: 0.8995, val_d2_accuracy: 0.8527\n",
      "Epoch 17, val_d1 loss: 0.2419, val_d2 loss: 0.4610, val_d1_f1: 0.8950, val_d2_f1: 0.4620, val_d1_accuracy: 0.8954, val_d2_accuracy: 0.8510\n",
      "Epoch 18, val_d1 loss: 0.2505, val_d2 loss: 0.4345, val_d1_f1: 0.8955, val_d2_f1: 0.4601, val_d1_accuracy: 0.8959, val_d2_accuracy: 0.8523\n",
      "Epoch 19, val_d1 loss: 0.2554, val_d2 loss: 0.4557, val_d1_f1: 0.8893, val_d2_f1: 0.4648, val_d1_accuracy: 0.8897, val_d2_accuracy: 0.8530\n",
      "Epoch 20, val_d1 loss: 0.2433, val_d2 loss: 0.4427, val_d1_f1: 0.8984, val_d2_f1: 0.4668, val_d1_accuracy: 0.8987, val_d2_accuracy: 0.8523\n",
      "Epoch 21, val_d1 loss: 0.2121, val_d2 loss: 0.4637, val_d1_f1: 0.9199, val_d2_f1: 0.4665, val_d1_accuracy: 0.9200, val_d2_accuracy: 0.8513\n",
      "Epoch 22, val_d1 loss: 0.2506, val_d2 loss: 0.4332, val_d1_f1: 0.8912, val_d2_f1: 0.4628, val_d1_accuracy: 0.8918, val_d2_accuracy: 0.8537\n",
      "Epoch 23, val_d1 loss: 0.2490, val_d2 loss: 0.5225, val_d1_f1: 0.8924, val_d2_f1: 0.4649, val_d1_accuracy: 0.8928, val_d2_accuracy: 0.8534\n",
      "Epoch 24, val_d1 loss: 0.2282, val_d2 loss: 0.4537, val_d1_f1: 0.9075, val_d2_f1: 0.4620, val_d1_accuracy: 0.9077, val_d2_accuracy: 0.8510\n",
      "Epoch 25, val_d1 loss: 0.2324, val_d2 loss: 0.4322, val_d1_f1: 0.9153, val_d2_f1: 0.4662, val_d1_accuracy: 0.9154, val_d2_accuracy: 0.8503\n",
      "Epoch 26, val_d1 loss: 0.2276, val_d2 loss: 0.4329, val_d1_f1: 0.9119, val_d2_f1: 0.4714, val_d1_accuracy: 0.9121, val_d2_accuracy: 0.8473\n",
      "Epoch 27, val_d1 loss: 0.2162, val_d2 loss: 0.5075, val_d1_f1: 0.9194, val_d2_f1: 0.4599, val_d1_accuracy: 0.9195, val_d2_accuracy: 0.8517\n",
      "Epoch 28, val_d1 loss: 0.2449, val_d2 loss: 0.4628, val_d1_f1: 0.9039, val_d2_f1: 0.4622, val_d1_accuracy: 0.9041, val_d2_accuracy: 0.8517\n",
      "Epoch 29, val_d1 loss: 0.2309, val_d2 loss: 0.5328, val_d1_f1: 0.9168, val_d2_f1: 0.4598, val_d1_accuracy: 0.9169, val_d2_accuracy: 0.8510\n",
      "Epoch 30, val_d1 loss: 0.2004, val_d2 loss: 0.4419, val_d1_f1: 0.9307, val_d2_f1: 0.4740, val_d1_accuracy: 0.9308, val_d2_accuracy: 0.8487\n",
      "Epoch 31, val_d1 loss: 0.2079, val_d2 loss: 0.4238, val_d1_f1: 0.9233, val_d2_f1: 0.4619, val_d1_accuracy: 0.9233, val_d2_accuracy: 0.8507\n",
      "Epoch 32, val_d1 loss: 0.2268, val_d2 loss: 0.4278, val_d1_f1: 0.9078, val_d2_f1: 0.4736, val_d1_accuracy: 0.9079, val_d2_accuracy: 0.8530\n",
      "Epoch 33, val_d1 loss: 0.2084, val_d2 loss: 0.4795, val_d1_f1: 0.9284, val_d2_f1: 0.4716, val_d1_accuracy: 0.9285, val_d2_accuracy: 0.8534\n",
      "Epoch 34, val_d1 loss: 0.1990, val_d2 loss: 0.5182, val_d1_f1: 0.9279, val_d2_f1: 0.4647, val_d1_accuracy: 0.9279, val_d2_accuracy: 0.8527\n",
      "Epoch 35, val_d1 loss: 0.2107, val_d2 loss: 0.4843, val_d1_f1: 0.9142, val_d2_f1: 0.4736, val_d1_accuracy: 0.9144, val_d2_accuracy: 0.8530\n",
      "Epoch 36, val_d1 loss: 0.2116, val_d2 loss: 0.4606, val_d1_f1: 0.9189, val_d2_f1: 0.4665, val_d1_accuracy: 0.9190, val_d2_accuracy: 0.8513\n",
      "Epoch 37, val_d1 loss: 0.2162, val_d2 loss: 0.4761, val_d1_f1: 0.9186, val_d2_f1: 0.4711, val_d1_accuracy: 0.9187, val_d2_accuracy: 0.8520\n",
      "Epoch 38, val_d1 loss: 0.2149, val_d2 loss: 0.4595, val_d1_f1: 0.9137, val_d2_f1: 0.4683, val_d1_accuracy: 0.9138, val_d2_accuracy: 0.8503\n",
      "Epoch 39, val_d1 loss: 0.2191, val_d2 loss: 0.4893, val_d1_f1: 0.9166, val_d2_f1: 0.4658, val_d1_accuracy: 0.9167, val_d2_accuracy: 0.8493\n",
      "Epoch 40, val_d1 loss: 0.2088, val_d2 loss: 0.4342, val_d1_f1: 0.9139, val_d2_f1: 0.4664, val_d1_accuracy: 0.9141, val_d2_accuracy: 0.8510\n",
      "Epoch 41, val_d1 loss: 0.2102, val_d2 loss: 0.4439, val_d1_f1: 0.9098, val_d2_f1: 0.4761, val_d1_accuracy: 0.9100, val_d2_accuracy: 0.8487\n",
      "Epoch 42, val_d1 loss: 0.2134, val_d2 loss: 0.4459, val_d1_f1: 0.9132, val_d2_f1: 0.4623, val_d1_accuracy: 0.9133, val_d2_accuracy: 0.8520\n",
      "Epoch 43, val_d1 loss: 0.2276, val_d2 loss: 0.4298, val_d1_f1: 0.9259, val_d2_f1: 0.4788, val_d1_accuracy: 0.9259, val_d2_accuracy: 0.8503\n",
      "Epoch 44, val_d1 loss: 0.2319, val_d2 loss: 0.4361, val_d1_f1: 0.9199, val_d2_f1: 0.4638, val_d1_accuracy: 0.9200, val_d2_accuracy: 0.8497\n",
      "Epoch 45, val_d1 loss: 0.2427, val_d2 loss: 0.4376, val_d1_f1: 0.9207, val_d2_f1: 0.4738, val_d1_accuracy: 0.9208, val_d2_accuracy: 0.8480\n",
      "Epoch 46, val_d1 loss: 0.2050, val_d2 loss: 0.4499, val_d1_f1: 0.9297, val_d2_f1: 0.4731, val_d1_accuracy: 0.9297, val_d2_accuracy: 0.8517\n",
      "Epoch 47, val_d1 loss: 0.2233, val_d2 loss: 0.4580, val_d1_f1: 0.9111, val_d2_f1: 0.4711, val_d1_accuracy: 0.9113, val_d2_accuracy: 0.8520\n",
      "Epoch 48, val_d1 loss: 0.2081, val_d2 loss: 0.4489, val_d1_f1: 0.9246, val_d2_f1: 0.4625, val_d1_accuracy: 0.9246, val_d2_accuracy: 0.8527\n",
      "Epoch 49, val_d1 loss: 0.2182, val_d2 loss: 0.4354, val_d1_f1: 0.9100, val_d2_f1: 0.4727, val_d1_accuracy: 0.9103, val_d2_accuracy: 0.8507\n",
      "Epoch 50, val_d1 loss: 0.2165, val_d2 loss: 0.4414, val_d1_f1: 0.9233, val_d2_f1: 0.4728, val_d1_accuracy: 0.9233, val_d2_accuracy: 0.8510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "lstm_model = LSTM_Label_Classifier(vocab_size=5000, emb_size=128, hidden_size=64).to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training the LSTM_Label_Classifier\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, train_f1, train_accuracy = train_one_epoch_nn(lstm_model, train_loader, criterion, device)\n",
    "    val_d1_loss, val_d1_f1, val_d1_accuracy = evaluate(lstm_model, val_d1_loader, criterion, device)\n",
    "    val_d2_loss, val_d2_f1, val_d2_accuracy = evaluate(lstm_model, val_d2_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}, val_d1 loss: {val_d1_loss:.4f}, val_d2 loss: {val_d2_loss:.4f}, val_d1_f1: {val_d1_f1:.4f}, val_d2_f1: {val_d2_f1:.4f}, val_d1_accuracy: {val_d1_accuracy:.4f}, val_d2_accuracy: {val_d2_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "530601ae-35de-4236-b27f-5cef38e7d5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictions = predict(lstm_model, test_loader, device)\n",
    "class Predictor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        predictions = []  # Store the predictions\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                txt = batch.to(self.device)  # Assuming each batch is just a tensor of token ids\n",
    "                logits = self.model(txt)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = torch.round(probs).int()  # Convert probabilities to binary labels (0 or 1)\n",
    "                predictions.extend(preds.tolist())\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2db5ade7-e18a-4d5c-ba22-f67fe7966c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the Predictor\n",
    "predictor = Predictor(lstm_model, device)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = predictor.predict(test_loader)\n",
    "\n",
    "test_ids = test['id'].tolist()\n",
    "\n",
    "assert len(test_ids) == len(test_predictions)\n",
    "\n",
    "# Write the results to the CSV\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,class\\n\")  # header line\n",
    "    for test_id, prediction in zip(test_ids, test_predictions):\n",
    "        f.write(f\"{test_id},{prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a261c6-da2b-4da5-b14e-304eab221169",
   "metadata": {},
   "source": [
    "## TF-IFD Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2cde47df-533e-44db-a9fd-98ff89267d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFIDF_Dataset(Dataset):\n",
    "    def __init__(self, tfidf_matrix, labels=None, max_length=MAX_LENGTH, labels_column='label'):\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.labels_column = labels_column\n",
    "        self.has_labels = labels is not None  # Check if labels are provided\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) if self.has_labels else self.tfidf_matrix.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tfidf_vector = self.tfidf_matrix[idx].toarray()  # Convert to dense array\n",
    "        tfidf_vector = tfidf_vector.squeeze()  # Remove the extra dimension\n",
    "        tfidf_vector = torch.tensor(tfidf_vector, dtype=torch.float32)\n",
    "\n",
    "        if self.has_labels:\n",
    "            label = torch.tensor(self.labels[idx])\n",
    "            return tfidf_vector, label\n",
    "        else:\n",
    "            return tfidf_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f4037961-2e99-4c28-9cfe-7ebb3f084b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TFIDF_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_prob=0.8):\n",
    "        super(TFIDF_Classifier, self).__init__()\n",
    "\n",
    "        # A sequential container for the classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Add dropout layer with the specified dropout probability\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),  # Add another dropout layer\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tfidf_vector):\n",
    "        output = self.classifier(tfidf_vector)\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1ea71213-131d-444e-bae7-d08e11858d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "train_combined.reset_index(drop=True, inplace=True)\n",
    "val_d1.reset_index(drop=True, inplace=True)\n",
    "val_d2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "all_text_data = pd.concat([train_combined['text'], val_d1['text'], val_d2['text']], axis=0)\n",
    "\n",
    "\n",
    "# Convert the pandas Series to a list of strings\n",
    "all_text_data_list = all_text_data.tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer with your desired MAX_FEATURES\n",
    "tfidf_matrix_all = tfidf_vectorizer.fit_transform(all_text_data)\n",
    "\n",
    "# Transform individual datasets\n",
    "tfidf_matrix_train_combined = tfidf_vectorizer.transform(train_combined['text'])\n",
    "tfidf_matrix_val_d1 = tfidf_vectorizer.transform(val_d1['text'])\n",
    "tfidf_matrix_val_d2 = tfidf_vectorizer.transform(val_d2['text'])\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(test['text'])\n",
    "\n",
    "# Create instances of TFIDF_Dataset for each dataset\n",
    "tfidf_train_combined_db = TFIDF_Dataset(tfidf_matrix_train_combined, train_combined['label'])\n",
    "tfidf_val_d1_db = TFIDF_Dataset(tfidf_matrix_val_d1, val_d1['label'])\n",
    "tfidf_val_d2_db = TFIDF_Dataset(tfidf_matrix_val_d2, val_d2['label'])\n",
    "tfidf_test_db = TFIDF_Dataset(tfidf_matrix_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoader instances for each dataset\n",
    "tfidf_train_loader = DataLoader(tfidf_train_combined_db, batch_size=batch_size, shuffle=True)\n",
    "tfidf_val_d1_loader = DataLoader(tfidf_val_d1_db, batch_size=batch_size, shuffle=False)\n",
    "tfidf_val_d2_loader = DataLoader(tfidf_val_d2_db, batch_size=batch_size, shuffle=False)\n",
    "tfidf_test_loader = DataLoader(tfidf_test_db, batch_size=batch_size, shuffle=False)  # No need to shuffle the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4aa6bdf9-d339-4769-bbed-678e5ee0193a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_d1 loss: 0.6910, val_d2 loss: 0.4994, val_d1_f1: 0.3333, val_d2_f1: 0.4611, val_d1_accuracy: 0.5000, val_d2_accuracy: 0.8557\n",
      "Epoch 2, val_d1 loss: 0.5712, val_d2 loss: 0.4570, val_d1_f1: 0.3401, val_d2_f1: 0.4611, val_d1_accuracy: 0.5031, val_d2_accuracy: 0.8557\n",
      "Epoch 3, val_d1 loss: 0.4224, val_d2 loss: 0.4575, val_d1_f1: 0.8296, val_d2_f1: 0.4743, val_d1_accuracy: 0.8305, val_d2_accuracy: 0.8302\n",
      "Epoch 4, val_d1 loss: 0.3412, val_d2 loss: 0.4722, val_d1_f1: 0.8746, val_d2_f1: 0.4802, val_d1_accuracy: 0.8746, val_d2_accuracy: 0.8168\n",
      "Epoch 5, val_d1 loss: 0.3040, val_d2 loss: 0.4828, val_d1_f1: 0.8843, val_d2_f1: 0.4862, val_d1_accuracy: 0.8844, val_d2_accuracy: 0.8101\n",
      "Epoch 6, val_d1 loss: 0.2863, val_d2 loss: 0.4896, val_d1_f1: 0.8866, val_d2_f1: 0.4918, val_d1_accuracy: 0.8867, val_d2_accuracy: 0.8070\n",
      "Epoch 7, val_d1 loss: 0.2728, val_d2 loss: 0.4975, val_d1_f1: 0.8935, val_d2_f1: 0.4987, val_d1_accuracy: 0.8936, val_d2_accuracy: 0.7973\n",
      "Epoch 8, val_d1 loss: 0.2707, val_d2 loss: 0.4993, val_d1_f1: 0.8889, val_d2_f1: 0.5010, val_d1_accuracy: 0.8890, val_d2_accuracy: 0.8060\n",
      "Epoch 9, val_d1 loss: 0.2639, val_d2 loss: 0.5050, val_d1_f1: 0.8958, val_d2_f1: 0.5046, val_d1_accuracy: 0.8959, val_d2_accuracy: 0.7966\n",
      "Epoch 10, val_d1 loss: 0.2662, val_d2 loss: 0.5039, val_d1_f1: 0.8908, val_d2_f1: 0.5008, val_d1_accuracy: 0.8908, val_d2_accuracy: 0.8034\n",
      "Epoch 11, val_d1 loss: 0.2630, val_d2 loss: 0.5070, val_d1_f1: 0.8895, val_d2_f1: 0.5081, val_d1_accuracy: 0.8895, val_d2_accuracy: 0.7983\n",
      "Epoch 12, val_d1 loss: 0.2622, val_d2 loss: 0.5103, val_d1_f1: 0.8887, val_d2_f1: 0.5063, val_d1_accuracy: 0.8887, val_d2_accuracy: 0.7973\n",
      "Epoch 13, val_d1 loss: 0.2615, val_d2 loss: 0.5141, val_d1_f1: 0.8872, val_d2_f1: 0.5049, val_d1_accuracy: 0.8872, val_d2_accuracy: 0.7930\n",
      "Epoch 14, val_d1 loss: 0.2615, val_d2 loss: 0.5186, val_d1_f1: 0.8882, val_d2_f1: 0.5071, val_d1_accuracy: 0.8882, val_d2_accuracy: 0.7906\n",
      "Epoch 15, val_d1 loss: 0.2640, val_d2 loss: 0.5207, val_d1_f1: 0.8854, val_d2_f1: 0.5051, val_d1_accuracy: 0.8854, val_d2_accuracy: 0.7913\n",
      "Epoch 16, val_d1 loss: 0.2646, val_d2 loss: 0.5259, val_d1_f1: 0.8862, val_d2_f1: 0.5067, val_d1_accuracy: 0.8862, val_d2_accuracy: 0.7879\n",
      "Epoch 17, val_d1 loss: 0.2653, val_d2 loss: 0.5317, val_d1_f1: 0.8872, val_d2_f1: 0.5148, val_d1_accuracy: 0.8872, val_d2_accuracy: 0.7846\n",
      "Epoch 18, val_d1 loss: 0.2689, val_d2 loss: 0.5351, val_d1_f1: 0.8859, val_d2_f1: 0.5137, val_d1_accuracy: 0.8859, val_d2_accuracy: 0.7846\n",
      "Epoch 19, val_d1 loss: 0.2694, val_d2 loss: 0.5443, val_d1_f1: 0.8877, val_d2_f1: 0.5174, val_d1_accuracy: 0.8877, val_d2_accuracy: 0.7785\n",
      "Epoch 20, val_d1 loss: 0.2742, val_d2 loss: 0.5453, val_d1_f1: 0.8882, val_d2_f1: 0.5151, val_d1_accuracy: 0.8882, val_d2_accuracy: 0.7832\n"
     ]
    }
   ],
   "source": [
    "#Initialise the training process\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "max_length = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "model_tfidf=TFIDF_Classifier(input_size=max_length,hidden_size=300).to(device)\n",
    "\n",
    "optimizer=optim.Adam(model_tfidf.parameters(),lr=0.00005)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Training the Label_Classifier\n",
    "num_epoch=20\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, train_f1, train_accuracy = train_one_epoch_nn(model_tfidf, tfidf_train_loader, criterion, device)\n",
    "    val_d1_loss, val_d1_f1 ,val_d1_accuracy = evaluate(model_tfidf, tfidf_val_d1_loader, criterion, device)\n",
    "    val_d2_loss, val_d2_f1, val_d2_accuracy = evaluate(model_tfidf, tfidf_val_d2_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}, val_d1 loss: {val_d1_loss:.4f}, val_d2 loss: {val_d2_loss:.4f}, val_d1_f1: {val_d1_f1:.4f}, val_d2_f1: {val_d2_f1:.4f}, val_d1_accuracy: {val_d1_accuracy:.4f}, val_d2_accuracy: {val_d2_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e157c7d-1208-4d95-8c91-ba3b09535fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict_dataloader(label_models, tfidf_models, label_loader, tfidf_loader, device):\n",
    "    total_probabilities = []\n",
    "\n",
    "    # Assuming the dataloaders provide data in the same order\n",
    "    for (label_batch, tfidf_batch) in zip(label_loader, tfidf_loader):\n",
    "        X_label = label_batch[0].to(device)\n",
    "        X_tfidf = tfidf_batch[0].to(device)\n",
    "\n",
    "        batch_probabilities = torch.zeros(X_label.size(0), device=device)\n",
    "\n",
    "        for model in label_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probabilities = torch.sigmoid(model(X_label).squeeze())\n",
    "                batch_probabilities += probabilities\n",
    "\n",
    "        for model in tfidf_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probabilities = torch.sigmoid(model(X_tfidf).squeeze()) \n",
    "                batch_probabilities += probabilities\n",
    "\n",
    "        avg_batch_probabilities = batch_probabilities / (len(label_models) + len(tfidf_models))\n",
    "        total_probabilities.extend(avg_batch_probabilities.tolist())\n",
    "\n",
    "    return total_probabilities\n",
    "\n",
    "def ensemble_predict_unlabeled_dataloader(label_models, tfidf_models=None, label_loader, tfidf_loader=None, device):\n",
    "    total_probabilities = []\n",
    "\n",
    "    \n",
    "    for (X_label, X_tfidf) in zip(label_loader, tfidf_loader):\n",
    "        X_label = X_label.to(device)  \n",
    "        X_tfidf = X_tfidf.to(device)  \n",
    "\n",
    "        batch_probabilities = torch.zeros(X_label.size(0), device=device)\n",
    "\n",
    "        for model in label_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probabilities = torch.sigmoid(model(X_label).squeeze())  \n",
    "                batch_probabilities += probabilities\n",
    "\n",
    "        for model in tfidf_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probabilities = torch.sigmoid(model(X_tfidf).squeeze())  \n",
    "                batch_probabilities += probabilities\n",
    "\n",
    "        avg_batch_probabilities = batch_probabilities / (len(label_models) + len(tfidf_models))\n",
    "        total_probabilities.extend(avg_batch_probabilities.tolist())\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    predicted_labels = [1 if prob > 0.5 else 0 for prob in total_probabilities]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "549f96a0-1d25-41c1-8217-e95f6f0dfebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_labels_test = ensemble_predict_unlabeled_dataloader(label_models, tfidf_models, test_loader, tfidf_test_loader, device)\n",
    "\n",
    "\n",
    "# Assuming the test set dataframe has a column named 'id' for IDs\n",
    "test_ids = test['id'].tolist()\n",
    "\n",
    "assert len(test_ids) == len(predicted_labels_test)\n",
    "\n",
    "# Write the results to the CSV\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,class\\n\")  # header line\n",
    "    for test_id, prediction in zip(test_ids, predicted_labels_test):\n",
    "        f.write(f\"{test_id},{prediction}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
